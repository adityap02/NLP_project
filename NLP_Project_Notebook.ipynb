{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "## Final Project\n",
        "\n",
        "Team:\\\n",
        "Aditya Porwal\\\n",
        "Samruddha Deshmukh\\\n",
        "Sahana Narasipura Vasudevarao\\\n",
        "Manas Bhilare\\\n",
        "Ankit Hiremath\n"
      ],
      "metadata": {
        "id": "US-nWeIn_ena"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt\n",
        "!pip list --format=freeze > requirements.txt"
      ],
      "metadata": {
        "id": "_9F9P8TrEhMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L9GJtpPguDo"
      },
      "outputs": [],
      "source": [
        "from google.colab import output as colab_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT-YBZKbgWSd"
      },
      "source": [
        "## Installing Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwpnEVMdgY6R"
      },
      "outputs": [],
      "source": [
        "!pip install trax\n",
        "\n",
        "colab_output.clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CChWzW-rEHVb",
        "outputId": "8f4573a3-452d-42a2-ca54-9dc588e4f654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=70)\n",
        "import trax\n",
        "from trax.fastmath import numpy as jnp\n",
        "from trax import layers as tl\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from google.colab import files\n",
        "import requests\n",
        "np.set_printoptions(threshold=sys.maxsize)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEL2rvaHRWP4"
      },
      "source": [
        "<a name='1'></a>\n",
        "# Part 1: Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK_noUxS4XBm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a3bb1b5-7798-4dbe-e604-13f7f5d139b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir 'data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJBjvRX0pdby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c18abafc-b037-4b8e-ffc4-434cac272a8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Load successful!\n"
          ]
        }
      ],
      "source": [
        "#cnn dataset\n",
        "!wget 'https://drive.usercontent.google.com/download?id=1-5yLc1-XSlRosxsK1TZ0m2P39yi1fG9z&export=download&authuser=4&confirm=t&uuid=2f0692df-5263-4db7-87a1-871e84449d7a&at=APZUnTXUiCYoOIedEXbwRJRkbg7u:1714786393602' -O 'cnn_dailymail.zip'\n",
        "!unzip -o 'cnn_dailymail.zip' -d 'data'\n",
        "!rm cnn_dailymail.zip\n",
        "colab_output.clear()\n",
        "print('Data Load successful!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkBbwz2gYvkN",
        "outputId": "7e75ba63-2b54-4dbd-cb5f-f71ffa52cdfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Load successful!\n"
          ]
        }
      ],
      "source": [
        "# Vocab\n",
        "!wget 'https://drive.usercontent.google.com/download?id=1wKyQIcCc4KVUb-VTscKcbVXkzcn5kcew&export=download&authuser=4&confirm=t&uuid=9dde6379-f698-466e-86b5-b3cb73d16264&at=APZUnTVKhwwT_VoBf-BoujW5saEw:1714786446282' -O 'vocabs.zip'\n",
        "!unzip -o 'vocabs.zip' -d 'data'\n",
        "!rm vocabs.zip\n",
        "colab_output.clear()\n",
        "print('Data Load successful!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "EEgPLfalQXMz",
        "outputId": "47e255f7-492e-4b5e-cec2-cdaa05ffd9d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         id  \\\n",
              "0  0001d1afc246a7964130f43ae940af6bc6c57f01   \n",
              "1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n",
              "2  00027e965c8264c35cc1bc55556db388da82b07f   \n",
              "3  0002c17436637c4fe1837c935c04de47adb18e9a   \n",
              "4  0003ad6ef0c37534f80b55b4235108024b407f0b   \n",
              "\n",
              "                                             article  \\\n",
              "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
              "1  (CNN) -- Ralph Mata was an internal affairs li...   \n",
              "2  A drunk driver who killed a young woman in a h...   \n",
              "3  (CNN) -- With a breezy sweep of his pen Presid...   \n",
              "4  Fleetwood are the only team still to have a 10...   \n",
              "\n",
              "                                          highlights  \n",
              "0  Bishop John Folda, of North Dakota, is taking ...  \n",
              "1  Criminal complaint: Cop used his role to help ...  \n",
              "2  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
              "3  Nina dos Santos says Europe must be ready to a...  \n",
              "4  Fleetwood top of League One after 2-0 win at S...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f5cad134-8482-4688-b047-95e139b3c152\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>article</th>\n",
              "      <th>highlights</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0001d1afc246a7964130f43ae940af6bc6c57f01</td>\n",
              "      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n",
              "      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n",
              "      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n",
              "      <td>Criminal complaint: Cop used his role to help ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00027e965c8264c35cc1bc55556db388da82b07f</td>\n",
              "      <td>A drunk driver who killed a young woman in a h...</td>\n",
              "      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0002c17436637c4fe1837c935c04de47adb18e9a</td>\n",
              "      <td>(CNN) -- With a breezy sweep of his pen Presid...</td>\n",
              "      <td>Nina dos Santos says Europe must be ready to a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0003ad6ef0c37534f80b55b4235108024b407f0b</td>\n",
              "      <td>Fleetwood are the only team still to have a 10...</td>\n",
              "      <td>Fleetwood top of League One after 2-0 win at S...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5cad134-8482-4688-b047-95e139b3c152')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f5cad134-8482-4688-b047-95e139b3c152 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f5cad134-8482-4688-b047-95e139b3c152');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-381dcc46-4268-4758-ade8-a658a0de945c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-381dcc46-4268-4758-ade8-a658a0de945c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-381dcc46-4268-4758-ade8-a658a0de945c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_data_df"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "train_data_df = pd.read_csv('data/cnn_dailymail/train.csv')\n",
        "test_data_df = pd.read_csv('data/cnn_dailymail/test.csv')\n",
        "train_data_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv1ZsyWwQOxR"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTsV_z2MsiZx"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "## 1.1 Preprocessing\n",
        "\n",
        "Applying preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0-gIHiVtsiCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8259a2ab-6744-4b4f-edef-244c54eaad2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:5: DeprecationWarning: invalid escape sequence '\\w'\n"
          ]
        }
      ],
      "source": [
        "# Tree bank Word Tokenizer\n",
        "# This is used to tokenize the document\n",
        "class custom_tbw:\n",
        "  def __init__(self):\n",
        "     self.re_period_pattern = re.compile('\\w+\\.$')\n",
        "     treebankword_tokenize = TreebankWordTokenizer()\n",
        "     self.treebankword_tokenizer = treebankword_tokenize.tokenize\n",
        "\n",
        "  def tokenizer(self, input_text):\n",
        "    pre_token = self.treebankword_tokenizer(input_text)\n",
        "    out_token = []\n",
        "\n",
        "    for token in pre_token:\n",
        "      if self.re_period_pattern.match(token): # Checking if the word ends with .\n",
        "        out_token.append(token[:-1])\n",
        "        out_token.append(token[-1])\n",
        "      else:\n",
        "        out_token.append(token)\n",
        "    return out_token\n",
        "\n",
        "class custom_text_processing(custom_tbw):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self._def_init__()\n",
        "\n",
        "  def _def_init__(self):\n",
        "\n",
        "    self.negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
        "    self.sw_l = nltk.corpus.stopwords.words('english')\n",
        "    self.sw_l = [sw for sw in self.sw_l if sw not in self.negationwords]\n",
        "    self.punctuation_l = ['.', ',', '--', '\"', \"'\"]\n",
        "    self.sw_dict = {sw.lower():False for sw in self.sw_l}\n",
        "    self.punt_dict = {sw.lower():False for sw in self.punctuation_l}\n",
        "    # Wordnet lemmantizer:\n",
        "    self.wnl = nltk.WordNetLemmatizer()\n",
        "    self.stop_pat = r'\\b(?:{})\\b'.format('|'.join(self.sw_l))\n",
        "\n",
        "  def ReturnCleanText(self, text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\W+|_\", ' ', text)\n",
        "    return re.sub(self.stop_pat, '', text)\n",
        "\n",
        "  # tokenize\n",
        "  def text_tokenization(self, in_text):\n",
        "    return self.tokenizer(in_text)\n",
        "\n",
        "  def lowertoken(self, in_token):\n",
        "    return [t.lower() for t in in_token]\n",
        "\n",
        "  # Lemmantize tokens\n",
        "  def wnl_lemmatize(self, in_token):\n",
        "    return [self.wnl.lemmatize(token) for token in in_token]\n",
        "\n",
        "  #check stop words\n",
        "  def is_not_sw(self, in_token):\n",
        "    return self.sw_dict.get(in_token.lower(),True)\n",
        "\n",
        "  def is_not_punct(self, in_token):\n",
        "    return self.punt_dict.get(in_token.lower(),True)\n",
        "\n",
        "  def remove_stopwords(self, in_token):\n",
        "    return [token for token in in_token if self.is_not_sw(token.lower())]\n",
        "\n",
        "  def remove_punctuation(self, in_token):\n",
        "    return [token for token in in_token if self.is_not_punct(token.lower())]\n",
        "\n",
        "  def clean_text_processing(self, in_text):\n",
        "    text = self.ReturnCleanText(in_text)\n",
        "    tokens = self.text_tokenization(text)\n",
        "    tokens = self.lowertoken(tokens)\n",
        "    tokens = self.wnl_lemmatize(tokens)\n",
        "    out_text = ' '.join(tokens)\n",
        "    return out_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "KLFpeRFktZ7U"
      },
      "outputs": [],
      "source": [
        "text_processer = custom_text_processing()\n",
        "clean_text_processor = text_processer.clean_text_processing\n",
        "def process_text(x):\n",
        "  clean_text_processor(x)\n",
        "  return bytes(str(x), 'utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "pvsX58fWTQKx",
        "outputId": "0ff774a6-9208-45ab-8cc3-5e65f90effbf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         id  \\\n",
              "0  0001d1afc246a7964130f43ae940af6bc6c57f01   \n",
              "1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n",
              "2  00027e965c8264c35cc1bc55556db388da82b07f   \n",
              "3  0002c17436637c4fe1837c935c04de47adb18e9a   \n",
              "4  0003ad6ef0c37534f80b55b4235108024b407f0b   \n",
              "\n",
              "                                                text  \\\n",
              "0  b\"By . Associated Press . PUBLISHED: . 14:11 E...   \n",
              "1  b'(CNN) -- Ralph Mata was an internal affairs ...   \n",
              "2  b\"A drunk driver who killed a young woman in a...   \n",
              "3  b\"(CNN) -- With a breezy sweep of his pen Pres...   \n",
              "4  b\"Fleetwood are the only team still to have a ...   \n",
              "\n",
              "                                             summary  \n",
              "0  b'Bishop John Folda, of North Dakota, is takin...  \n",
              "1  b'Criminal complaint: Cop used his role to hel...  \n",
              "2  b\"Craig Eccleston-Todd, 27, had drunk at least...  \n",
              "3  b\"Nina dos Santos says Europe must be ready to...  \n",
              "4  b'Fleetwood top of League One after 2-0 win at...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-48d9e51e-43f4-486c-a247-6468913676c8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0001d1afc246a7964130f43ae940af6bc6c57f01</td>\n",
              "      <td>b\"By . Associated Press . PUBLISHED: . 14:11 E...</td>\n",
              "      <td>b'Bishop John Folda, of North Dakota, is takin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n",
              "      <td>b'(CNN) -- Ralph Mata was an internal affairs ...</td>\n",
              "      <td>b'Criminal complaint: Cop used his role to hel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00027e965c8264c35cc1bc55556db388da82b07f</td>\n",
              "      <td>b\"A drunk driver who killed a young woman in a...</td>\n",
              "      <td>b\"Craig Eccleston-Todd, 27, had drunk at least...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0002c17436637c4fe1837c935c04de47adb18e9a</td>\n",
              "      <td>b\"(CNN) -- With a breezy sweep of his pen Pres...</td>\n",
              "      <td>b\"Nina dos Santos says Europe must be ready to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0003ad6ef0c37534f80b55b4235108024b407f0b</td>\n",
              "      <td>b\"Fleetwood are the only team still to have a ...</td>\n",
              "      <td>b'Fleetwood top of League One after 2-0 win at...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48d9e51e-43f4-486c-a247-6468913676c8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-48d9e51e-43f4-486c-a247-6468913676c8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-48d9e51e-43f4-486c-a247-6468913676c8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-974f093f-9cd7-4b79-a183-066b55b56f9d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-974f093f-9cd7-4b79-a183-066b55b56f9d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-974f093f-9cd7-4b79-a183-066b55b56f9d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "processed_train_data_df"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# Preporcessing 1\n",
        "# Rename columns\n",
        "rename_col_dict = {'article': 'text', 'highlights': 'summary'}\n",
        "processed_train_data_df = train_data_df.rename(columns=rename_col_dict)\n",
        "processed_test_data_df = test_data_df.rename(columns=rename_col_dict)\n",
        "\n",
        "# applying bytes on training data\n",
        "processed_train_data_df['text'] = processed_train_data_df['text'].apply(lambda x: process_text(str(x)))\n",
        "processed_train_data_df['summary'] = processed_train_data_df['summary'].apply(lambda x: process_text(str(x)))\n",
        "\n",
        "# applying bytes on test data\n",
        "processed_test_data_df['text'] = processed_test_data_df['text'].apply(lambda x: process_text(str(x)))\n",
        "processed_test_data_df['summary'] = processed_test_data_df['summary'].apply(lambda x: process_text(str(x)))\n",
        "\n",
        "# Preview\n",
        "processed_train_data_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGwEwY1qsvGp"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEmblgeNQPnk"
      },
      "source": [
        "Making Text stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "eiGzs-JJRsbK"
      },
      "outputs": [],
      "source": [
        "def data_stream(data_df, n:int = 0, col_n:dict={'text': 'text', 'summary':'summary'}):\n",
        "  c = 0\n",
        "  for index, row in data_df.iterrows():\n",
        "    c += 1\n",
        "    if n != 0 and c>n:\n",
        "        break\n",
        "    yield row[col_n['text']], row[col_n['summary']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "4YN7_6WfVFKb"
      },
      "outputs": [],
      "source": [
        "# making a text stream for training\n",
        "train_stream_fn = data_stream(processed_train_data_df)\n",
        "test_stream_fn = data_stream(processed_test_data_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "test_text, test_summary = next(test_stream_fn)\n",
        "print(test_text)\n",
        "print('')\n",
        "print(test_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSRdMZbWuR8t",
        "outputId": "b757ff36-b1f1-46cc-c6b8-754859677856"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"Ever noticed how plane seats appear to be getting smaller and smaller? With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable - it's putting our health and safety in danger. More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger? This week, a U.S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes, it doesn't stipulate a minimum amount of space for humans. 'In a world where animals have more rights to space and food than humans,' said Charlie Leocha, consumer representative on the committee.\\xc2\\xa0'It is time that the DOT and FAA take a stand for humane treatment of passengers.' But could crowding on planes lead to more serious issues than fighting for space in the overhead lockers, crashing elbows and seat back kicking? Tests conducted by the FAA use planes with a 31 inch pitch, a standard which on some airlines has decreased . Many economy seats on United Airlines have 30 inches of room, while some airlines offer as little as 28 inches . Cynthia Corbertt, a human factors researcher with the Federal Aviation Administration, that it conducts tests on how quickly passengers can leave a plane. But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased, reported the Detroit News. The distance between two seats from one point on a seat to the same point on the seat behind it is known as the pitch. While most airlines stick to a pitch of 31 inches or above, some fall below this. While United Airlines has 30 inches of space, Gulf Air economy seats have between 29 and 32 inches, Air Asia offers 29 inches and Spirit Airlines offers just 28 inches. British Airways has a seat pitch of 31 inches, while easyJet has 29 inches, Thomson's short haul seat pitch is 28 inches, and Virgin Atlantic's is 30-31.\"\n",
            "\n",
            "b'Experts question if  packed out planes are putting passengers at risk .\\nU.S consumer advisory group says minimum space must be stipulated .\\nSafety tests conducted on planes with more leg room than airlines offer .'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf98b0n0UL0F"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djTiSLcaNFGa",
        "outputId": "f164b7a5-3108-4b22-cb4f-06dc0275543e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[129, 39, 1151, 354, 17976, 81, 2, 23472, 58, 2, 186, 19696, 909, 320, 191, 28, 1311, 16852, 1700, 1]\n",
            "We will be using Transformer, encoder, and Decoder to make a text summarization \n",
            "\n",
            "We will be using Transformer, encoder, and Decoder to make a text\n",
            "summarization<EOS>\n"
          ]
        }
      ],
      "source": [
        "def tokenizer(input_str, EOS=1):\n",
        "    inputs =  next(trax.data.tokenize(iter([input_str]),vocab_dir='data/vocabs/',vocab_file='summarize32k.subword.subwords'))\n",
        "    return list(inputs) + [EOS]\n",
        "\n",
        "def detokenizer(integers):\n",
        "    # List of ints (tokenized sentence) to string\n",
        "    s = trax.data.detokenize(integers,vocab_dir='data/vocabs/',vocab_file='summarize32k.subword.subwords')\n",
        "    return wrapper.fill(s)\n",
        "test_s = 'We will be using Transformer, encoder, and Decoder to make a text summarization'\n",
        "tokens=tokenizer(test_s)\n",
        "print(tokens)\n",
        "print(test_s,'\\n')\n",
        "print(detokenizer(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WvhaFbCRWQS"
      },
      "source": [
        "## 1.3 Preprocessing for Language Models:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "c4rgPxYSRWQS"
      },
      "outputs": [],
      "source": [
        "sep = 0\n",
        "eos = 1\n",
        "def preprocess(stream):\n",
        "  for (text, summary) in stream:\n",
        "    joint = np.array(list(text) + [eos, sep] + list(summary) + [sep])\n",
        "    mask = [0] * (len(list(text)) + 2) + [1] * (len(list(summary)) + 1) # Accounting for sep and sep\n",
        "    yield joint, joint, np.array(mask)\n",
        "\n",
        "# combining data preprocessing steps into a pipeline\n",
        "input_pipeline = trax.data.Serial(\n",
        "  #Tokenizes -> Pipenline -> Filters longer than 2048\n",
        "  trax.data.Tokenize(vocab_dir='data/vocabs/', vocab_file='summarize32k.subword.subwords')  , preprocess  , trax.data.FilterByLength(2048))\n",
        "processed_train_stream = input_pipeline(train_stream_fn)\n",
        "processed_test_stream = input_pipeline(test_stream_fn)\n",
        "train_input, train_target, train_mask = next(processed_train_stream)\n",
        "\n",
        "assert sum((train_input - train_target)**2) == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKFoGsUKSa_I",
        "outputId": "772a3618-5c83-4f2c-efb2-fe0a52536916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single example mask:\n",
            "\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] \n",
            "\n",
            " ------------------- \n",
            "\n",
            "\n",
            "Single example train input:\n",
            "\n",
            " [  567   379 13550   574   379  7226  5182  3047  6611   136  4601     3\n",
            "   373   180   253 16958     4     2   393   560   429 11969 28081   379\n",
            "  9720 22449  3590  4601     3   271   180  1513 16958     4     2   393\n",
            "   560   429   379     9  8762   527   213  4035   273  1448 21905   132\n",
            "   438  8597    23  4537  4840  3808   527   687   339   132  4035   273\n",
            "     2  1711  4629  1283   186  6331 13708  3313   320   213 17758  6067\n",
            "    27  6879   132   532   493   186   263   560     3     9   205  1404\n",
            "   676    23  1956   163 14443   527  4713  1019  1815  1779  2561   409\n",
            "  2758   186   436 18808     3  3963   240  7773  1000     8 12370    21\n",
            "    12   527   213  4035   273  1448 21905   132   438  8597    23  4537\n",
            "  4840  3808   527   687   339   132  4035   273     2  1711  4629  1283\n",
            "   186  6331 13708  3313   320   213 17758  6067    27   379   303 13347\n",
            "  3951  1700  2122  3860 11984    20 12685   371   465   213   993   229\n",
            "   529     2    35  1631  1006   103     7     5   294   320 18340   101\n",
            "   320   213   498  4713     3     9 20596  1595    78  2613   285  3963\n",
            "   240  7773  1000   229   892    55   236   102   144 17935  1248 17758\n",
            "  6067    27     3     9 20596   465    22 23062   213  5890   123 25724\n",
            "    17   837   192  7449    28  1900  1019  3713 20483 14089   132  2060\n",
            "   220   984     3  7099  6334  1387   527 17758  6067    27   377 19215\n",
            "     2 11721 16532     2  1201   527  7219 20266     2  8225  1304   186\n",
            " 16891 12016  7564 26835    26     3  4035   273  1448 21905   132   438\n",
            "  8597     8 12370    21    12   229  4872   213  8762   229   698  2104\n",
            "     1     0  3963   240  7773  1000     2   527   438  8597     2   229\n",
            "   892    55   236   102   144 17935 16346 27439  6774  1628    69 23062\n",
            "   213  5890   123 25724    17   837   132  2060 16346 27439  6774  1628\n",
            "   526   339   132  4035   273     2  1711  4629  1283   186  6331 13708\n",
            "  3313   143    18    46  4537  2104     0] \n",
            "\n",
            " ------------------- \n",
            "\n",
            "\n",
            "Single example:\n",
            "\n",
            " By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | .\n",
            "UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo\n",
            "Catholic Diocese in North Dakota has exposed potentially hundreds of\n",
            "church members in Fargo, Grand Forks and Jamestown to the hepatitis A\n",
            "virus in late September and early October. The state Health Department\n",
            "has issued an advisory of exposure for anyone who attended five\n",
            "churches and took communion. Bishop John Folda (pictured) of the Fargo\n",
            "Catholic Diocese in North Dakota has exposed potentially hundreds of\n",
            "church members in Fargo, Grand Forks and Jamestown to the hepatitis A\n",
            ". State Immunization Program Manager Molly Howell says the risk is\n",
            "low, but officials feel it's important to alert people to the possible\n",
            "exposure. The diocese announced on Monday that Bishop John Folda is\n",
            "taking time off after being diagnosed with hepatitis A. The diocese\n",
            "says he contracted the infection through contaminated food while\n",
            "attending a conference for newly ordained bishops in Italy last month.\n",
            "Symptoms of hepatitis A include fever, tiredness, loss of appetite,\n",
            "nausea and abdominal discomfort. Fargo Catholic Diocese in North\n",
            "Dakota (pictured) is where the bishop is located .<EOS><pad>BishopJohn\n",
            "Folda, of North Dakota, is taking time off after being diagnosed . He\n",
            "contracted the infection through contaminated food in Italy . Church\n",
            "members in Fargo, Grand Forks and Jamestown could have been exposed\n",
            ".<pad> \n",
            "\n",
            " ------------------- \n",
            "\n",
            "\n",
            "Single example train target:\n",
            "\n",
            " [  567   379 13550   574   379  7226  5182  3047  6611   136  4601     3\n",
            "   373   180   253 16958     4     2   393   560   429 11969 28081   379\n",
            "  9720 22449  3590  4601     3   271   180  1513 16958     4     2   393\n",
            "   560   429   379     9  8762   527   213  4035   273  1448 21905   132\n",
            "   438  8597    23  4537  4840  3808   527   687   339   132  4035   273\n",
            "     2  1711  4629  1283   186  6331 13708  3313   320   213 17758  6067\n",
            "    27  6879   132   532   493   186   263   560     3     9   205  1404\n",
            "   676    23  1956   163 14443   527  4713  1019  1815  1779  2561   409\n",
            "  2758   186   436 18808     3  3963   240  7773  1000     8 12370    21\n",
            "    12   527   213  4035   273  1448 21905   132   438  8597    23  4537\n",
            "  4840  3808   527   687   339   132  4035   273     2  1711  4629  1283\n",
            "   186  6331 13708  3313   320   213 17758  6067    27   379   303 13347\n",
            "  3951  1700  2122  3860 11984    20 12685   371   465   213   993   229\n",
            "   529     2    35  1631  1006   103     7     5   294   320 18340   101\n",
            "   320   213   498  4713     3     9 20596  1595    78  2613   285  3963\n",
            "   240  7773  1000   229   892    55   236   102   144 17935  1248 17758\n",
            "  6067    27     3     9 20596   465    22 23062   213  5890   123 25724\n",
            "    17   837   192  7449    28  1900  1019  3713 20483 14089   132  2060\n",
            "   220   984     3  7099  6334  1387   527 17758  6067    27   377 19215\n",
            "     2 11721 16532     2  1201   527  7219 20266     2  8225  1304   186\n",
            " 16891 12016  7564 26835    26     3  4035   273  1448 21905   132   438\n",
            "  8597     8 12370    21    12   229  4872   213  8762   229   698  2104\n",
            "     1     0  3963   240  7773  1000     2   527   438  8597     2   229\n",
            "   892    55   236   102   144 17935 16346 27439  6774  1628    69 23062\n",
            "   213  5890   123 25724    17   837   132  2060 16346 27439  6774  1628\n",
            "   526   339   132  4035   273     2  1711  4629  1283   186  6331 13708\n",
            "  3313   143    18    46  4537  2104     0] \n",
            "\n",
            " ------------------- \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f'Single example mask:\\n\\n {train_mask} \\n\\n ------------------- \\n\\n')\n",
        "\n",
        "print(f'Single example train input:\\n\\n {train_input} \\n\\n ------------------- \\n\\n')\n",
        "\n",
        "print(f'Single example:\\n\\n {detokenizer(train_input)} \\n\\n ------------------- \\n\\n')\n",
        "\n",
        "print(f'Single example train target:\\n\\n {train_target} \\n\\n ------------------- \\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-wb38tvXbu4"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4sDS1WIVaYG"
      },
      "source": [
        "<a name='1.3'></a>\n",
        "\n",
        "## 1.4 Batching with bucketing\n",
        "\n",
        "using bucketing to create batches of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "oqj1NsbERWQX"
      },
      "outputs": [],
      "source": [
        "boundaries =  [128, 256, 512, 1024, 2048]\n",
        "batch_sizes = [16, 8, 4, 2, 1]\n",
        "\n",
        "train_batch_stream = trax.data.BucketByLength(boundaries, batch_sizes)(processed_train_stream)\n",
        "# Test\n",
        "test_batch_stream = trax.data.BucketByLength(boundaries, batch_sizes)(processed_test_stream)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6M5OA8QRWQb",
        "outputId": "b763be1a-0ffe-4c68-e114-b8572ba573ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2048)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "input_batch, _, mask_batch = next(train_batch_stream)\n",
        "input_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjNOlljxTGuQ",
        "outputId": "0351bcdf-3d18-46b2-ec96-a98f6ff80e1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   27 23176  4694  1779  1343    28   506  1091   132    28   570     6\n",
            "    78  7124   192 14454    15  3570  2067    23    46 26133    17  1019\n",
            "   635    91     3  5349 23421   494     6 10487     2   728     2  1353\n",
            "  3156   278  1838    28   736   809    28 13481  7511    22   625    28\n",
            "  1311  2396     3   187    22  1353  1510   181 16146  1049   320   103\n",
            "     2    22 26563   651   467   213   826   192  3156  1262    28 13131\n",
            "     4   186 16949    17    71 12319  6604   828 29725     4     5  1081\n",
            "  1083   213    54   138     3  5349 23421   494     6 10487     2   728\n",
            "     8   346    12  1353   354    15  3570  2067  7511    22 24497   570\n",
            "     6    78    71   213  1081   144  3360   691 12319  6604   828     2\n",
            "   705     8   231    24   305   710   272  1838    68  6341   379     9\n",
            "   570     6    78  7124   436   219   132   560   429     3   368 23421\n",
            "   494     6 10487     7     5  1081  1353 10874 20919   217     8 12370\n",
            "    21    12  2713   127 23421   494     6 10487    40 23176   809   518\n",
            "   150   181   290  3892   275   527  8947   171  1269   936   213  9025\n",
            "     3    69  1353   233  8272   527  6056   583   691  4398  3156   809\n",
            " 14507  5429   812  7356     3  3622  6604   828     2    28   705     6\n",
            "   104     6   292 15004   181 29725     4     5 21961  1838 10687    45\n",
            "     2 11985   527 11907  5364     2    40    43  1383   213  2801  1248\n",
            "  1078   809    28 13481    35    40    19 23176   116  4016     2   864\n",
            "   127     3   305  1353  3156 17775 12979  3095   186    77  1353   669\n",
            " 27439  6050 13459  1628  1290   131   143    18   757   320  2501   213\n",
            " 25725 29725     2    41   969     3 16978  1822  9855  1962     2 17347\n",
            "    16     2   127  4601 27439  6050 13459  1628  5349 23421   494     6\n",
            " 10487 29725     4     5  3156  2868   132   213 15191   583   527    28\n",
            "   506  1091     2 12319  6604   828     2    28   583   285   143    18\n",
            "    46 13488 23707  6050 13459  1628   368 23421   494     6 10487   436\n",
            "   213   884   320  3429    61    15  3570  2067  6715  3156   186     2\n",
            "   673  1510   181 16146  1049   320   824  1311  2396     2  1353    90\n",
            " 15438    17   285    22  2214   320 17950    28   346     6   650 13131\n",
            "     4     2  7228   213  1052   763   314    71   213  2358   527  3622\n",
            "  6604   828 29725     4     5 18352  2398  1081     3  3622  6604   828\n",
            "  1353  7214   213 19839   277   527    68 27439  9275  1628 12320  5403\n",
            "  9242  5590  2385    35   710   272  1838    68  6341   132  2642 11969\n",
            " 27439  6050 13459  1628  3622  6604   828   669 27884     4    40 27872\n",
            "   391    28  5302   531  2504   527    68     3   305  1353    43  4925\n",
            "   278   523  1383   163 20812  2801  1248  1078   186  1353  3156 17775\n",
            " 12979  3095 23707  6050 13459  1628   305    40  5945   320  1242    68\n",
            "  1078  7511   131   540   278   320  8916   285   131    40  2362 15627\n",
            "     3  1561  1078  8075   114   369  1613  1838    68   102    41  7584\n",
            "    17   458 23707  6050 13459  1628  3622  6604   828 29725     4     5\n",
            "   583   132    97  2861  6107 17946     5   213  6349   527   354    28\n",
            "   650     6   475  3570  2067  6715  3156  4172 29725   391  2713    25\n",
            "  3630   320   245 17388   181  1884  4140  1838 23421   494     6 10487\n",
            "  1820     2    35   132  4140   329   926   102   213  5556    22  1353\n",
            "    86 25070   918   155   213  6700     6  2057  3602     3     9  4038\n",
            "  2256  1248   864   285    22    62    18    46    95   213  3602   809\n",
            "   213    55    15   651  6866  4604   279  1205  3622  6604   828 29725\n",
            "     4     5  2498 12320  5403  9242  5590  2385    78    28   826   542\n",
            " 15902  3569     2 11985   527 11907  5364     2    78   560   253     2\n",
            "   429     3   405  2067   992  1606    22  1353    43 17997   595   239\n",
            "   213    55   527   213  7124     3  6753  1565  8120   479     2  1838\n",
            " 12887 26509 21380   328 29725     4     5  1839 25725  2694  1676     2\n",
            "   127  3611   871  5784  1435  1248 12319     7     5   228   809   824\n",
            "    55     3   305    40    46    64  1248  1078   809    28 13481   132\n",
            " 15010  7301   285  2801     2    35    40    19    40   116  4016  1782\n",
            "   871  2694  1606   285    77  1353  1290   131   143    18   757   320\n",
            "  2501   213 25725   186  8075   114   103   919    68    68   177  1782\n",
            "   368 23421   494     6 10487    40   346   126   132 15902  3569   186\n",
            "  1326  1248  1078   809    28 13481  4872    22  6005  6929   809   518\n",
            "   150   320   290  3892   275   527  7468    81     3    69 12402     7\n",
            "    26   209   346   213 13481   320   955   278  7511   213 25725  1841\n",
            "   809   239   128    10  3229  2535  1782   129  8198     7    26   217\n",
            "   320   245 17388   181  1884  4140  1838   134  1820   186   849  1884\n",
            "   576   329   926   102   213 25725  1606    22  1353 25070   918   155\n",
            "   213  3602     2    51  2253    22    62    18    46    95   213  3602\n",
            "   809   213    55   527   213 25725   186   132 13040  2398    61   592\n",
            "     2   213  4038  2256  1782     9   641   527    15  2067   992  1606\n",
            "   285    22  1353 17997   595    78    15  2067   239   213    55   527\n",
            "   213 25725    90   103     7     5  1232   761   824    62    43    18\n",
            "  3625   320    15  4398  3156   186  1201   527   490  2002 23421   494\n",
            "     6 10487  1353   233  8272   527  6056   583   691  4398  3156   355\n",
            "    28  2145   809 14507  5429   812     8 12370    21    12    69   969\n",
            "  3611   368 23421   494     6 10487    39   169  3263   635    91   936\n",
            "  5892     2    35 12319     7     5   228    18   913    68  8232  1782\n",
            "    13  1525   824    39   191   101   362  3060   171  6642   116  4016\n",
            "   186  1269   936   213  9025     2   181   354    28  2067   640    41\n",
            "     7   165    78   213   826  1782     9 26024   527  6700  3156   186\n",
            "  3156  6715   354    28  3570  2067  1435  3787     3  2994  1779   952\n",
            "   320   124    90   993  3736    28  3537    55   132  2173     3    56\n",
            "   347  6335   141  7270 15191   213  4472   527 16972   595    97 23891\n",
            "  6412    49  1151 20327 27439  6050 13459  1628   368 23421   494     6\n",
            " 10487    39   169  3263   635    91   936  5892     2    35 12319 29725\n",
            "     4     5   228    18   913    68  1019   545     3    13  1525   824\n",
            "    39   191   101   362  3060   171  6642   116  4016   186  1269   936\n",
            "   213  9025     2   181   354    28  2067   640    41 29725     4   165\n",
            "    78   213   826     3    56   347  6335   141  7270 15191   213  4472\n",
            "   527 16972   595    97 23891  6412    49  1151  4172 29725   391 23421\n",
            "   494     6 10487     2   527 14735     2 11985   527 11907  5364     2\n",
            "  1353    43 24306  5831  4461  1838  3156  1019  1223    91 27439  9275\n",
            "  1628   102  1480    22    39    18   320   976   163  2008   165     6\n",
            "  1166    10     1     0  5349 23421   494     6 10487     2   728     2\n",
            "    40 23176   809   518   150  3892   275   171  3156  1081 16346 27439\n",
            "  6774  1628  5670   354  2067  7511    22 26563   651   467   826   132\n",
            " 15902  3569     2 11985   527 11907  5364 16346 27439  6774  1628  3481\n",
            "  3094   570     6    78    71   705     6   104     6   292 12319  6604\n",
            "   828     7     5  1081     2  1779   710   132  2642 16346 27439  6774\n",
            "  1628  2713   476    22    62    18    46    95   904  6700     6  2057\n",
            "  3602   809    55   527  7124 16346 27439  6774  1628    69  1353   233\n",
            "  8272   809 14507  5429   812   527  6056   583   691  4398  3156  2104\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "A drunk driver who killed a young woman in a head-on crash while\n",
            "checking his mobile phone has been jailed for six years. Craig\n",
            "Eccleston-Todd, 27, was driving home from a night at a pub when he\n",
            "received a text message. As he was reading or replying to it, he\n",
            "veered across the road while driving round a bend and smashed into\n",
            "Rachel Titley’s car coming the other way. Craig Eccleston-Todd, 27\n",
            "(left) was using his mobile phone when he crashed head-on into the car\n",
            "being driven by Rachel Titley, 28 (right). She died later from her\n",
            "injuries . The head-on crash took place in October 2013. Mr Eccleston-\n",
            "Todd's car was barely recognisable (pictured) Police said Eccleston-\n",
            "Todd had drunk at least three or four pints of beer before getting\n",
            "behind the wheel. He was found guilty of causing death by dangerous\n",
            "driving at Portsmouth Crown Court yesterday. Miss Titley, a 28-year-\n",
            "old solicitor’s clerk from Cowes, Isle of Wight, had also spent the\n",
            "evening with friends at a pub but had not drunk any alcohol, police\n",
            "said. She was driving responsibly and there was ‘nothing she could\n",
            "have done to avoid the collision’, they added. Lindsay Pennell,\n",
            "prosecuting, said: ‘Craig Eccleston-Todd’s driving resulted in the\n",
            "tragic death of a young woman, Rachel Titley, a death that could have\n",
            "been avoided. ‘Mr Eccleston-Todd took the decision to pick up his\n",
            "mobile phone whilst driving and, either reading or replying to this\n",
            "text message, was so distracted that he failed to negotiate a left-\n",
            "hand bend, crossing the central white line into the path of Miss\n",
            "Titley’s oncoming car. Miss Titley was pulled the wreckage of\n",
            "her Daihatsu Cuore but died later from her injuries in hospital .\n",
            "‘Miss Titley [had] a bright future ahead of her. She was also\n",
            "returning home having spent an enjoyable evening with friends and was\n",
            "driving responsibly. ‘She had arranged to contact her friends when she\n",
            "got home to confirm that she had arrived safely. Her friends sadly\n",
            "never heard from her after they parted company. ‘Miss Titley’s death\n",
            "in these circumstances reiterates the danger of using a hand-held\n",
            "mobile phone whilst driving.’ Police were unable to take breath or\n",
            "blood tests from Eccleston-Todd immediately, but in tests several\n",
            "hours after the accident he was only marginally under the drink-drive\n",
            "limit. The judge agreed with police that he would have been over the\n",
            "limit at the time his red Citroen hit Miss Titley’s blue Daihatsu\n",
            "Cuore on a road near Yarmouth, Isle of Wight, on October 11, 2013. His\n",
            "phone records showed he was also texting around the time of the crash.\n",
            "PC Mark Furse, from Hampshire constabulary’s serious collision\n",
            "investigation unit, said: 'Our thoughts are with Rachel's family at\n",
            "this time. She had been out with friends at a pub in Shalfleet that\n",
            "evening, but had not had any alcohol. 'Our investigation showed that\n",
            "there was nothing she could have done to avoid the collision and sadly\n",
            "it cost her her life. 'Mr Eccleston-Todd had left work in Yarmouth and\n",
            "met with friends at a pub where he drank at least three to four pints\n",
            "of lager. He hadn't long left the pub to return home when the\n",
            "collision occurred at around 9.30pm. 'We weren't able to take breath\n",
            "or blood tests from him immediately and although blood taken several\n",
            "hours after the collision showed he was marginally under the limit, we\n",
            "maintain he would have been over the limit at the time of the\n",
            "collision and in summing up today, the judge agreed. 'The analysis of\n",
            "his phone records showed that he was texting on his phone around the\n",
            "time of the collision so it's highly likely this would also have\n",
            "contributed to his dangerous driving and loss of control.' Eccleston-\n",
            "Todd was found guilty of causing death by dangerous driving following\n",
            "a trial at Portsmouth Crown Court (pictured) He added: 'Mr Eccleston-\n",
            "Todd will now spend six years behind bars, but Rachel's family have\n",
            "lost her forever. 'I hope this will make people think twice before\n",
            "drinking any alcohol and getting behind the wheel, or using a phone\n",
            "once they're on the road. 'The dangers of drink driving and driving\n",
            "whilst using a mobile phone are obvious. Those who continue to do so\n",
            "risk spending a substantial time in prison. This case highlights just\n",
            "how tragic the consequences of committing these offences can be.' ‘Mr\n",
            "Eccleston-Todd will now spend six years behind bars, but Rachel’s\n",
            "family have lost her for ever. I hope this will make people think\n",
            "twice before drinking any alcohol and getting behind the wheel, or\n",
            "using a phone once they’re on the road. This case highlights just how\n",
            "tragic the consequences of committing these offences can be.’\n",
            "Eccleston-Todd, of Newport, Isle of Wight, was also disqualified from\n",
            "driving for eight years after which he will have to complete an\n",
            "extended re-test.<EOS><pad>CraigEccleston-Todd, 27, had drunk at least\n",
            "three pints before driving car . Was using phone when he veered across\n",
            "road in Yarmouth, Isle of Wight . Crashed head-on into 28-year-old\n",
            "Rachel Titley's car, who died in hospital . Police say he would have\n",
            "been over legal drink-drive limit at time of crash . He was found\n",
            "guilty at Portsmouth Crown Court of causing death by dangerous driving\n",
            ".<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ]
        }
      ],
      "source": [
        "# print integer values\n",
        "print(input_batch[0])\n",
        "print('\\n------------------------------------------------------------------------\\n')\n",
        "print(detokenizer(input_batch[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bu05ZwbWTE6P",
        "outputId": "d7af4cdf-c9ca-4a7e-a335-b3a8243eedc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article:\n",
            "\n",
            " A drunk driver who killed a young woman in a head-on crash while\n",
            "checking his mobile phone has been jailed for six years. Craig\n",
            "Eccleston-Todd, 27, was driving home from a night at a pub when he\n",
            "received a text message. As he was reading or replying to it, he\n",
            "veered across the road while driving round a bend and smashed into\n",
            "Rachel Titley’s car coming the other way. Craig Eccleston-Todd, 27\n",
            "(left) was using his mobile phone when he crashed head-on into the car\n",
            "being driven by Rachel Titley, 28 (right). She died later from her\n",
            "injuries . The head-on crash took place in October 2013. Mr Eccleston-\n",
            "Todd's car was barely recognisable (pictured) Police said Eccleston-\n",
            "Todd had drunk at least three or four pints of beer before getting\n",
            "behind the wheel. He was found guilty of causing death by dangerous\n",
            "driving at Portsmouth Crown Court yesterday. Miss Titley, a 28-year-\n",
            "old solicitor’s clerk from Cowes, Isle of Wight, had also spent the\n",
            "evening with friends at a pub but had not drunk any alcohol, police\n",
            "said. She was driving responsibly and there was ‘nothing she could\n",
            "have done to avoid the collision’, they added. Lindsay Pennell,\n",
            "prosecuting, said: ‘Craig Eccleston-Todd’s driving resulted in the\n",
            "tragic death of a young woman, Rachel Titley, a death that could have\n",
            "been avoided. ‘Mr Eccleston-Todd took the decision to pick up his\n",
            "mobile phone whilst driving and, either reading or replying to this\n",
            "text message, was so distracted that he failed to negotiate a left-\n",
            "hand bend, crossing the central white line into the path of Miss\n",
            "Titley’s oncoming car. Miss Titley was pulled the wreckage of\n",
            "her Daihatsu Cuore but died later from her injuries in hospital .\n",
            "‘Miss Titley [had] a bright future ahead of her. She was also\n",
            "returning home having spent an enjoyable evening with friends and was\n",
            "driving responsibly. ‘She had arranged to contact her friends when she\n",
            "got home to confirm that she had arrived safely. Her friends sadly\n",
            "never heard from her after they parted company. ‘Miss Titley’s death\n",
            "in these circumstances reiterates the danger of using a hand-held\n",
            "mobile phone whilst driving.’ Police were unable to take breath or\n",
            "blood tests from Eccleston-Todd immediately, but in tests several\n",
            "hours after the accident he was only marginally under the drink-drive\n",
            "limit. The judge agreed with police that he would have been over the\n",
            "limit at the time his red Citroen hit Miss Titley’s blue Daihatsu\n",
            "Cuore on a road near Yarmouth, Isle of Wight, on October 11, 2013. His\n",
            "phone records showed he was also texting around the time of the crash.\n",
            "PC Mark Furse, from Hampshire constabulary’s serious collision\n",
            "investigation unit, said: 'Our thoughts are with Rachel's family at\n",
            "this time. She had been out with friends at a pub in Shalfleet that\n",
            "evening, but had not had any alcohol. 'Our investigation showed that\n",
            "there was nothing she could have done to avoid the collision and sadly\n",
            "it cost her her life. 'Mr Eccleston-Todd had left work in Yarmouth and\n",
            "met with friends at a pub where he drank at least three to four pints\n",
            "of lager. He hadn't long left the pub to return home when the\n",
            "collision occurred at around 9.30pm. 'We weren't able to take breath\n",
            "or blood tests from him immediately and although blood taken several\n",
            "hours after the collision showed he was marginally under the limit, we\n",
            "maintain he would have been over the limit at the time of the\n",
            "collision and in summing up today, the judge agreed. 'The analysis of\n",
            "his phone records showed that he was texting on his phone around the\n",
            "time of the collision so it's highly likely this would also have\n",
            "contributed to his dangerous driving and loss of control.' Eccleston-\n",
            "Todd was found guilty of causing death by dangerous driving following\n",
            "a trial at Portsmouth Crown Court (pictured) He added: 'Mr Eccleston-\n",
            "Todd will now spend six years behind bars, but Rachel's family have\n",
            "lost her forever. 'I hope this will make people think twice before\n",
            "drinking any alcohol and getting behind the wheel, or using a phone\n",
            "once they're on the road. 'The dangers of drink driving and driving\n",
            "whilst using a mobile phone are obvious. Those who continue to do so\n",
            "risk spending a substantial time in prison. This case highlights just\n",
            "how tragic the consequences of committing these offences can be.' ‘Mr\n",
            "Eccleston-Todd will now spend six years behind bars, but Rachel’s\n",
            "family have lost her for ever. I hope this will make people think\n",
            "twice before drinking any alcohol and getting behind the wheel, or\n",
            "using a phone once they’re on the road. This case highlights just how\n",
            "tragic the consequences of committing these offences can be.’\n",
            "Eccleston-Todd, of Newport, Isle of Wight, was also disqualified from\n",
            "driving for eight years after which he will have to complete an\n",
            "extended re-test.\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Summary:\n",
            "\n",
            " <pad>CraigEccleston-Todd, 27, had drunk at least\n",
            "three pints before driving car . Was using phone when he veered across\n",
            "road in Yarmouth, Isle of Wight . Crashed head-on into 28-year-old\n",
            "Rachel Titley's car, who died in hospital . Police say he would have\n",
            "been over legal drink-drive limit at time of crash . He was found\n",
            "guilty at Portsmouth Crown Court of causing death by dangerous driving\n",
            ".<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n",
            "><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ]
        }
      ],
      "source": [
        "# print the article and its summary\n",
        "print('Article:\\n\\n', detokenizer(input_batch[0]).split('<EOS>')[0])\n",
        "print('\\n------------------------------------------------------------------------\\n')\n",
        "print('Summary:\\n\\n', detokenizer(input_batch[0]).split('<EOS>')[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un8NHIRoj-1W"
      },
      "source": [
        "# Part 2: Summarization with transformer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AL6eP5u8ptN"
      },
      "source": [
        "## 2.1 Dot product attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "PW6fjyV0oD1f"
      },
      "outputs": [],
      "source": [
        "def create_tensor(t):\n",
        "    return jnp.array(t)\n",
        "\n",
        "\n",
        "def display_tensor(t, name):\n",
        "    print(f'{name} shape: {t.shape}\\n')\n",
        "    print(f'{t}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "kSauPt0NUl_o"
      },
      "outputs": [],
      "source": [
        "# DotProductAttention\n",
        "def DotProductAttention(query, key, value, mask):\n",
        "\n",
        "    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Embedding dimensions of q, k, v aren't all the same\"\n",
        "    depth = query.shape[-1]\n",
        "    dots = jnp.matmul(query, jnp.swapaxes(key, -1, -2)) / jnp.sqrt(depth)\n",
        "    if mask is not None: # The 'None' in this line does not need to be replaced\n",
        "        dots = jnp.where(mask, dots, jnp.full_like(dots, -1e9))\n",
        "    logsumexp = trax.fastmath.logsumexp(dots,axis=-1,keepdims=True)\n",
        "\n",
        "    dots = jnp.exp(dots-logsumexp)\n",
        "    attention = jnp.matmul(dots,value)\n",
        "    return attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y2PSiLVRWQ2"
      },
      "source": [
        "## 2.2 Causal Attention\n",
        "\n",
        "Implementing causal attention: multi-headed attention with a mask to attend only to words that occurred before.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "C_60sgx8oD1h"
      },
      "outputs": [],
      "source": [
        "# compute_attention_heads:\n",
        "def compute_attention_heads_closure(n_heads, dim_head):\n",
        "\n",
        "\n",
        "    def compute_attention_heads(x):\n",
        "        batch_size = x.shape[0]\n",
        "        len_seq = x.shape[1]\n",
        "        x = jnp.reshape(x,(batch_size, len_seq, n_heads, dim_head))\n",
        "        x = jnp.reshape(x,(-1, len_seq, dim_head))\n",
        "        return x\n",
        "    return compute_attention_heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "6S2llCGkoD1h"
      },
      "outputs": [],
      "source": [
        "# dot_product_self_attention :\n",
        "def dot_product_self_attention(q, k, v):\n",
        "    mask_size = q.shape[-2]\n",
        "    mask = jnp.tril(jnp.ones((1, mask_size, mask_size), dtype=jnp.bool_), k=0)\n",
        "    return DotProductAttention(q, k, v, mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "BB2sV6droD1i"
      },
      "outputs": [],
      "source": [
        "# compute_attention_output :\n",
        "def compute_attention_output_closure(n_heads, d_head):\n",
        "    def compute_attention_output(x):\n",
        "        seqlen = x.shape[1]\n",
        "        x = jnp.reshape(x,(-1, n_heads, seqlen, d_head))\n",
        "        x = jnp.transpose(x,(0,2,1,3))\n",
        "        return jnp.reshape(x, (-1, seqlen, n_heads * d_head))\n",
        "    return compute_attention_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwJj21hfoD1i"
      },
      "source": [
        "### Causal Attention Function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "B9Adn6DtRWRG"
      },
      "outputs": [],
      "source": [
        "# CausalAttention\n",
        "def CausalAttention(d_feature,\n",
        "                    n_heads,\n",
        "                    compute_attention_heads_closure=compute_attention_heads_closure,\n",
        "                    dot_product_self_attention=dot_product_self_attention,\n",
        "                    compute_attention_output_closure=compute_attention_output_closure,\n",
        "                    mode='train'):\n",
        "    assert d_feature % n_heads == 0\n",
        "    dim_head = d_feature // n_heads\n",
        "    ComputeAttentionHeads = tl.Fn('AttnHeads', compute_attention_heads_closure(n_heads, dim_head), n_out=1)\n",
        "    return tl.Serial(\n",
        "        tl.Branch(\n",
        "            [tl.Dense(n_units=d_feature), ComputeAttentionHeads], # queries\n",
        "            [tl.Dense(n_units=d_feature), ComputeAttentionHeads], # keys\n",
        "            [tl.Dense(n_units=d_feature), ComputeAttentionHeads],\n",
        "        ),\n",
        "\n",
        "        tl.Fn('DotProductAttn', dot_product_self_attention, n_out=1),\n",
        "        tl.Fn('AttnOutput', compute_attention_output_closure(n_heads, dim_head), n_out=1), # allow parallel\n",
        "        tl.Dense(d_feature) # Final dense layer\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVNAAuWcoD1i",
        "outputId": "32addec6-a0f1-4535-8230-cbb86c27d457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial[\n",
            "  Branch_out3[\n",
            "    [Dense_512, AttnHeads]\n",
            "    [Dense_512, AttnHeads]\n",
            "    [Dense_512, AttnHeads]\n",
            "  ]\n",
            "  DotProductAttn_in3\n",
            "  AttnOutput\n",
            "  Dense_512\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "print(CausalAttention(d_feature=512, n_heads=8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D9j27PbmTi0"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6zwtPjqRWRJ"
      },
      "source": [
        "<a name='2.3'></a>\n",
        "\n",
        "## 2.3 Transformer decoder block\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "gKOxnRbp1K5U"
      },
      "outputs": [],
      "source": [
        "# DecoderBlock\n",
        "def DecoderBlock(d_model, d_ff_n, n_heads,\n",
        "                 dropout_rate, mode, ff_activation):\n",
        "    causal_attention = CausalAttention(d_model, n_heads=n_heads, mode=mode)\n",
        "    feed_forward = [\n",
        "        tl.LayerNorm(),\n",
        "        tl.Dense(d_ff_n),\n",
        "        ff_activation(),\n",
        "        tl.Dropout(rate=dropout_rate,mode=mode),\n",
        "        tl.Dense(d_model),\n",
        "        tl.Dropout(rate=dropout_rate,mode=mode)\n",
        "    ]\n",
        "\n",
        "    return [\n",
        "      tl.Residual(\n",
        "          # Normalizing input layer\n",
        "          tl.LayerNorm(),\n",
        "          # causal attention\n",
        "          causal_attention,\n",
        "          # dropout\n",
        "          tl.Dropout(rate=dropout_rate,mode=mode)\n",
        "        ),\n",
        "      tl.Residual(\n",
        "          # Adding the feed forward block\n",
        "          feed_forward\n",
        "        ),\n",
        "      ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7mCsIL3oD1j",
        "outputId": "b0613a0e-7aa4-4e35-e423-7d4d0ad71691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Serial[\n",
            "  Branch_out2[\n",
            "    None\n",
            "    Serial[\n",
            "      LayerNorm\n",
            "      Serial[\n",
            "        Branch_out3[\n",
            "          [Dense_512, AttnHeads]\n",
            "          [Dense_512, AttnHeads]\n",
            "          [Dense_512, AttnHeads]\n",
            "        ]\n",
            "        DotProductAttn_in3\n",
            "        AttnOutput\n",
            "        Dense_512\n",
            "      ]\n",
            "      Dropout\n",
            "    ]\n",
            "  ]\n",
            "  Add_in2\n",
            "], Serial[\n",
            "  Branch_out2[\n",
            "    None\n",
            "    Serial[\n",
            "      LayerNorm\n",
            "      Dense_2048\n",
            "      Serial[\n",
            "        Relu\n",
            "      ]\n",
            "      Dropout\n",
            "      Dense_512\n",
            "      Dropout\n",
            "    ]\n",
            "  ]\n",
            "  Add_in2\n",
            "]]\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "print(DecoderBlock(d_model=512, d_ff_n=2048, n_heads=8, dropout_rate=0.1, mode='train', ff_activation=tl.Relu))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoFv-nfLRWRN"
      },
      "source": [
        "<a name='2.4'></a>\n",
        "## 2.4 Transformer Language Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "0yi4LJO1RWRS"
      },
      "outputs": [],
      "source": [
        "# TransformerLM\n",
        "def TransformerLM(vocab_size=33300, d_model=512, d_ff=2048, n_layers=6, n_heads=8, dropout=0.1, max_len=4096, mode='train', ff_activation=tl.Relu):\n",
        "    positional_encoder = [\n",
        "        tl.Embedding(vocab_size,d_model),\n",
        "        tl.Dropout(rate=dropout,mode=mode),\n",
        "        tl.PositionalEncoding(max_len=max_len,mode=mode)]\n",
        "\n",
        "    decoder_blocks = [DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)]\n",
        "\n",
        "    return tl.Serial(tl.ShiftRight(n_positions=1,mode=mode), positional_encoder, decoder_blocks, tl.LayerNorm(), tl.Dense(vocab_size), tl.LogSoftmax())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Uug2hN1oD1j",
        "outputId": "96787a04-26fd-4330-8284-31a26e40e902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_33300_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Serial[\n",
            "          Branch_out3[\n",
            "            [Dense_512, AttnHeads]\n",
            "            [Dense_512, AttnHeads]\n",
            "            [Dense_512, AttnHeads]\n",
            "          ]\n",
            "          DotProductAttn_in3\n",
            "          AttnOutput\n",
            "          Dense_512\n",
            "        ]\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Serial[\n",
            "          Relu\n",
            "        ]\n",
            "        Dropout\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  LayerNorm\n",
            "  Dense_33300\n",
            "  LogSoftmax\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "print(TransformerLM(n_layers=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRRKnoAdvmJ7"
      },
      "source": [
        "# Part 3: Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "gM2gpu4xvjtX"
      },
      "outputs": [],
      "source": [
        "from trax.supervised import training\n",
        "# train_model\n",
        "def training_loop(Transformer_model, data_stream, test_gen, n_steps_per_checkpoint: int = 10, model_dir: str = \"model/summary_transformer_model\"):\n",
        "\n",
        "  model_dir = os.path.expanduser(model_dir)\n",
        "  lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01)\n",
        "  train_task = training.TrainTask(\n",
        "    labeled_data=data_stream,\n",
        "    loss_layer=tl.CrossEntropyLoss(),\n",
        "    optimizer=trax.optimizers.Adam(0.01),\n",
        "    lr_schedule=lr_schedule,\n",
        "    n_steps_per_checkpoint= n_steps_per_checkpoint\n",
        "  )\n",
        "  test_task = training.EvalTask(labeled_data=test_gen, metrics=[tl.CrossEntropyLoss(), tl.Accuracy()])\n",
        "  loop = training.Loop(Transformer_model, train_task, eval_tasks=[test_task], output_dir=model_dir)\n",
        "  return loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_version_nm = 'summary_transformer_model_v1'\n",
        "steps = 1000\n",
        "create = True\n",
        "n_steps_per_checkpoint = 100\n",
        "model_path = '/content/model'\n",
        "model_dir = os.path.join(model_path, model_version_nm)\n",
        "\n",
        "if not os.path.exists(model_dir):\n",
        "  os.mkdir(model_path)\n",
        "\n",
        "New_transformer = TransformerLM(vocab_size=2000, d_model=32, d_ff=256, n_layers=3, n_heads=8, dropout=0.1, max_len=2048, ff_activation = tl.Relu)\n",
        "\n",
        "loop = training_loop(New_transformer, train_batch_stream, test_batch_stream, n_steps_per_checkpoint = n_steps_per_checkpoint, model_dir = model_dir )\n",
        "\n",
        "if not(create):\n",
        "  unzip_file(f'{model_dir}.zip')\n",
        "  loop.model.init_from_file(f'{model_dir}/model.pkl.gz', weights_only=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6Vgq6jNJ2O_",
        "outputId": "0d5f74c4-a305-41e2-c1d9-abe9d485cd4f"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/jax/_src/xla_bridge.py:1136: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7Kebj8xecTg",
        "outputId": "10e4cb7a-bdcf-4584-db60-bdc0bcf125fa"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "colab_output.clear()"
      ],
      "metadata": {
        "id": "xryGXr9jCehC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='3.2'></a>\n",
        "### 3.2 Training the Model"
      ],
      "metadata": {
        "id": "e-7FlD7my3nr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAECQiTvR2Tk",
        "outputId": "432300b5-6614-456c-ef9f-69644533f981"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trax/layers/base.py:851: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
            "  with gzip.GzipFile(fileobj=f, compresslevel=compresslevel) as gzipf:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 258672\n",
            "Step      1: Ran 1 train steps in 23.03 secs\n",
            "Step      1: train CrossEntropyLoss |  4.15438080\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trax/supervised/training.py:1249: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
            "  with gzip_lib.GzipFile(fileobj=f, compresslevel=2) as gzipf:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step      1: eval  CrossEntropyLoss |  3.97779417\n",
            "Step      1: eval          Accuracy |  0.00000000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trax/layers/base.py:851: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
            "  with gzip.GzipFile(fileobj=f, compresslevel=compresslevel) as gzipf:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step    100: Ran 99 train steps in 266.15 secs\n",
            "Step    100: train CrossEntropyLoss |  4.23030567\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trax/supervised/training.py:1249: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
            "  with gzip_lib.GzipFile(fileobj=f, compresslevel=2) as gzipf:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step    100: eval  CrossEntropyLoss |  3.87981224\n",
            "Step    100: eval          Accuracy |  0.03947368\n",
            "\n",
            "Step    200: Ran 100 train steps in 215.46 secs\n",
            "Step    200: train CrossEntropyLoss |  3.42259097\n",
            "Step    200: eval  CrossEntropyLoss |  3.15454340\n",
            "Step    200: eval          Accuracy |  0.04301075\n",
            "\n",
            "Step    300: Ran 100 train steps in 226.26 secs\n",
            "Step    300: train CrossEntropyLoss |  3.41211367\n",
            "Step    300: eval  CrossEntropyLoss |  3.21563435\n",
            "Step    300: eval          Accuracy |  0.02380952\n",
            "\n",
            "Step    400: Ran 100 train steps in 235.96 secs\n",
            "Step    400: train CrossEntropyLoss |  3.32570624\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trax/layers/base.py:851: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
            "  with gzip.GzipFile(fileobj=f, compresslevel=compresslevel) as gzipf:\n",
            "/usr/local/lib/python3.10/dist-packages/trax/supervised/training.py:1249: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
            "  with gzip_lib.GzipFile(fileobj=f, compresslevel=2) as gzipf:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step    400: eval  CrossEntropyLoss |  3.61801648\n",
            "Step    400: eval          Accuracy |  0.06250000\n",
            "\n",
            "Step    500: Ran 100 train steps in 214.82 secs\n",
            "Step    500: train CrossEntropyLoss |  3.24154043\n",
            "Step    500: eval  CrossEntropyLoss |  3.39850688\n",
            "Step    500: eval          Accuracy |  0.07954545\n"
          ]
        }
      ],
      "source": [
        "# Model Training\n",
        "additional_steps = 2000\n",
        "\n",
        "loop.run(additional_steps)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving\n",
        "# Zip and Download\n",
        "#zip_n_dl(model_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "lzUUSBKLJdam",
        "outputId": "83050ff9-a418-4d71-fa4b-00d1dd40af1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c84080cc-f5be-40f4-b109-dd5be2fcf905\", \"summary_transformer_model_v1.zip\", 3092536)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional Training (for more accuracy):\n",
        "additional_steps = 2000\n",
        "loop.run(additional_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEjmK1LtG07T",
        "outputId": "633ba3c8-9820-447f-f8be-4d0d7396d714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step   8020: Ran 10 train steps in 22.86 secs\n",
            "Step   8020: train CrossEntropyLoss |  2.84446287\n",
            "Step   8020: eval  CrossEntropyLoss |  2.84076333\n",
            "Step   8020: eval          Accuracy |  0.07563026\n",
            "\n",
            "Step   8030: Ran 10 train steps in 26.98 secs\n",
            "Step   8030: train CrossEntropyLoss |  2.89877772\n",
            "Step   8030: eval  CrossEntropyLoss |  2.84172583\n",
            "Step   8030: eval          Accuracy |  0.08888889\n",
            "\n",
            "Step   8040: Ran 10 train steps in 25.69 secs\n",
            "Step   8040: train CrossEntropyLoss |  2.85345316\n",
            "Step   8040: eval  CrossEntropyLoss |  3.31788206\n",
            "Step   8040: eval          Accuracy |  0.10714286\n",
            "\n",
            "Step   8050: Ran 10 train steps in 25.15 secs\n",
            "Step   8050: train CrossEntropyLoss |  2.72076464\n",
            "Step   8050: eval  CrossEntropyLoss |  3.16182280\n",
            "Step   8050: eval          Accuracy |  0.10714286\n",
            "\n",
            "Step   8060: Ran 10 train steps in 22.76 secs\n",
            "Step   8060: train CrossEntropyLoss |  2.78029776\n",
            "Step   8060: eval  CrossEntropyLoss |  2.78838396\n",
            "Step   8060: eval          Accuracy |  0.08965518\n",
            "\n",
            "Step   8070: Ran 10 train steps in 18.42 secs\n",
            "Step   8070: train CrossEntropyLoss |  2.85353494\n",
            "Step   8070: eval  CrossEntropyLoss |  2.50979495\n",
            "Step   8070: eval          Accuracy |  0.13782051\n",
            "\n",
            "Step   8080: Ran 10 train steps in 23.03 secs\n",
            "Step   8080: train CrossEntropyLoss |  2.90435719\n",
            "Step   8080: eval  CrossEntropyLoss |  2.70721579\n",
            "Step   8080: eval          Accuracy |  0.14880952\n",
            "\n",
            "Step   8090: Ran 10 train steps in 22.58 secs\n",
            "Step   8090: train CrossEntropyLoss |  2.74183583\n",
            "Step   8090: eval  CrossEntropyLoss |  2.49987864\n",
            "Step   8090: eval          Accuracy |  0.08125000\n",
            "\n",
            "Step   8100: Ran 10 train steps in 22.13 secs\n",
            "Step   8100: train CrossEntropyLoss |  2.99557185\n",
            "Step   8100: eval  CrossEntropyLoss |  3.23770308\n",
            "Step   8100: eval          Accuracy |  0.15540540\n",
            "\n",
            "Step   8110: Ran 10 train steps in 24.98 secs\n",
            "Step   8110: train CrossEntropyLoss |  2.77364993\n",
            "Step   8110: eval  CrossEntropyLoss |  2.84494734\n",
            "Step   8110: eval          Accuracy |  0.13375796\n",
            "\n",
            "Step   8120: Ran 10 train steps in 25.07 secs\n",
            "Step   8120: train CrossEntropyLoss |  2.88534784\n",
            "Step   8120: eval  CrossEntropyLoss |  2.83131552\n",
            "Step   8120: eval          Accuracy |  0.11290322\n",
            "\n",
            "Step   8130: Ran 10 train steps in 20.72 secs\n",
            "Step   8130: train CrossEntropyLoss |  3.01813769\n",
            "Step   8130: eval  CrossEntropyLoss |  2.46915817\n",
            "Step   8130: eval          Accuracy |  0.05405406\n",
            "\n",
            "Step   8140: Ran 10 train steps in 24.04 secs\n",
            "Step   8140: train CrossEntropyLoss |  2.79013562\n",
            "Step   8140: eval  CrossEntropyLoss |  2.55629063\n",
            "Step   8140: eval          Accuracy |  0.12605043\n",
            "\n",
            "Step   8150: Ran 10 train steps in 23.91 secs\n",
            "Step   8150: train CrossEntropyLoss |  2.85679674\n",
            "Step   8150: eval  CrossEntropyLoss |  2.64865279\n",
            "Step   8150: eval          Accuracy |  0.09473684\n",
            "\n",
            "Step   8160: Ran 10 train steps in 24.41 secs\n",
            "Step   8160: train CrossEntropyLoss |  2.75635386\n",
            "Step   8160: eval  CrossEntropyLoss |  2.27432203\n",
            "Step   8160: eval          Accuracy |  0.19148937\n",
            "\n",
            "Step   8170: Ran 10 train steps in 26.10 secs\n",
            "Step   8170: train CrossEntropyLoss |  2.92847776\n",
            "Step   8170: eval  CrossEntropyLoss |  2.79513884\n",
            "Step   8170: eval          Accuracy |  0.07812500\n",
            "\n",
            "Step   8180: Ran 10 train steps in 23.94 secs\n",
            "Step   8180: train CrossEntropyLoss |  2.75259423\n",
            "Step   8180: eval  CrossEntropyLoss |  3.06299114\n",
            "Step   8180: eval          Accuracy |  0.12380952\n",
            "\n",
            "Step   8190: Ran 10 train steps in 24.62 secs\n",
            "Step   8190: train CrossEntropyLoss |  2.92192507\n",
            "Step   8190: eval  CrossEntropyLoss |  2.98412657\n",
            "Step   8190: eval          Accuracy |  0.07894737\n",
            "\n",
            "Step   8200: Ran 10 train steps in 21.27 secs\n",
            "Step   8200: train CrossEntropyLoss |  2.85342288\n",
            "Step   8200: eval  CrossEntropyLoss |  2.52582169\n",
            "Step   8200: eval          Accuracy |  0.10328639\n",
            "\n",
            "Step   8210: Ran 10 train steps in 23.84 secs\n",
            "Step   8210: train CrossEntropyLoss |  2.79827309\n",
            "Step   8210: eval  CrossEntropyLoss |  2.06613898\n",
            "Step   8210: eval          Accuracy |  0.14516129\n",
            "\n",
            "Step   8220: Ran 10 train steps in 25.09 secs\n",
            "Step   8220: train CrossEntropyLoss |  2.88891482\n",
            "Step   8220: eval  CrossEntropyLoss |  3.43632507\n",
            "Step   8220: eval          Accuracy |  0.04729730\n",
            "\n",
            "Step   8230: Ran 10 train steps in 21.57 secs\n",
            "Step   8230: train CrossEntropyLoss |  2.82025552\n",
            "Step   8230: eval  CrossEntropyLoss |  2.69870186\n",
            "Step   8230: eval          Accuracy |  0.08287293\n",
            "\n",
            "Step   8240: Ran 10 train steps in 24.42 secs\n",
            "Step   8240: train CrossEntropyLoss |  2.81576467\n",
            "Step   8240: eval  CrossEntropyLoss |  2.49214792\n",
            "Step   8240: eval          Accuracy |  0.11111111\n",
            "\n",
            "Step   8250: Ran 10 train steps in 22.89 secs\n",
            "Step   8250: train CrossEntropyLoss |  2.94059873\n",
            "Step   8250: eval  CrossEntropyLoss |  2.30853796\n",
            "Step   8250: eval          Accuracy |  0.07446808\n",
            "\n",
            "Step   8260: Ran 10 train steps in 21.89 secs\n",
            "Step   8260: train CrossEntropyLoss |  2.75149870\n",
            "Step   8260: eval  CrossEntropyLoss |  3.18385196\n",
            "Step   8260: eval          Accuracy |  0.07142857\n",
            "\n",
            "Step   8270: Ran 10 train steps in 22.97 secs\n",
            "Step   8270: train CrossEntropyLoss |  2.78648281\n",
            "Step   8270: eval  CrossEntropyLoss |  2.50825238\n",
            "Step   8270: eval          Accuracy |  0.08496732\n",
            "\n",
            "Step   8280: Ran 10 train steps in 22.39 secs\n",
            "Step   8280: train CrossEntropyLoss |  3.01122332\n",
            "Step   8280: eval  CrossEntropyLoss |  3.23241830\n",
            "Step   8280: eval          Accuracy |  0.12352941\n",
            "\n",
            "Step   8290: Ran 10 train steps in 22.03 secs\n",
            "Step   8290: train CrossEntropyLoss |  2.72868371\n",
            "Step   8290: eval  CrossEntropyLoss |  3.29975057\n",
            "Step   8290: eval          Accuracy |  0.11363637\n",
            "\n",
            "Step   8300: Ran 10 train steps in 19.29 secs\n",
            "Step   8300: train CrossEntropyLoss |  2.67689085\n",
            "Step   8300: eval  CrossEntropyLoss |  3.41264296\n",
            "Step   8300: eval          Accuracy |  0.06024097\n",
            "\n",
            "Step   8310: Ran 10 train steps in 22.80 secs\n",
            "Step   8310: train CrossEntropyLoss |  3.10077500\n",
            "Step   8310: eval  CrossEntropyLoss |  2.70567131\n",
            "Step   8310: eval          Accuracy |  0.09917355\n",
            "\n",
            "Step   8320: Ran 10 train steps in 23.40 secs\n",
            "Step   8320: train CrossEntropyLoss |  2.83561540\n",
            "Step   8320: eval  CrossEntropyLoss |  2.47958875\n",
            "Step   8320: eval          Accuracy |  0.08387097\n",
            "\n",
            "Step   8330: Ran 10 train steps in 21.76 secs\n",
            "Step   8330: train CrossEntropyLoss |  2.84546995\n",
            "Step   8330: eval  CrossEntropyLoss |  2.68129444\n",
            "Step   8330: eval          Accuracy |  0.11347517\n",
            "\n",
            "Step   8340: Ran 10 train steps in 26.38 secs\n",
            "Step   8340: train CrossEntropyLoss |  2.77909231\n",
            "Step   8340: eval  CrossEntropyLoss |  2.46972847\n",
            "Step   8340: eval          Accuracy |  0.10303030\n",
            "\n",
            "Step   8350: Ran 10 train steps in 23.30 secs\n",
            "Step   8350: train CrossEntropyLoss |  2.95583344\n",
            "Step   8350: eval  CrossEntropyLoss |  2.68880630\n",
            "Step   8350: eval          Accuracy |  0.08904110\n",
            "\n",
            "Step   8360: Ran 10 train steps in 25.65 secs\n",
            "Step   8360: train CrossEntropyLoss |  2.70104647\n",
            "Step   8360: eval  CrossEntropyLoss |  2.94205213\n",
            "Step   8360: eval          Accuracy |  0.12903225\n",
            "\n",
            "Step   8370: Ran 10 train steps in 24.55 secs\n",
            "Step   8370: train CrossEntropyLoss |  2.90304041\n",
            "Step   8370: eval  CrossEntropyLoss |  2.40161395\n",
            "Step   8370: eval          Accuracy |  0.07500000\n",
            "\n",
            "Step   8380: Ran 10 train steps in 23.90 secs\n",
            "Step   8380: train CrossEntropyLoss |  2.69618845\n",
            "Step   8380: eval  CrossEntropyLoss |  2.66768599\n",
            "Step   8380: eval          Accuracy |  0.09090909\n",
            "\n",
            "Step   8390: Ran 10 train steps in 22.24 secs\n",
            "Step   8390: train CrossEntropyLoss |  2.83740067\n",
            "Step   8390: eval  CrossEntropyLoss |  2.77980518\n",
            "Step   8390: eval          Accuracy |  0.11111111\n",
            "\n",
            "Step   8400: Ran 10 train steps in 23.29 secs\n",
            "Step   8400: train CrossEntropyLoss |  2.75039911\n",
            "Step   8400: eval  CrossEntropyLoss |  2.64126420\n",
            "Step   8400: eval          Accuracy |  0.09210526\n",
            "\n",
            "Step   8410: Ran 10 train steps in 22.69 secs\n",
            "Step   8410: train CrossEntropyLoss |  2.62435818\n",
            "Step   8410: eval  CrossEntropyLoss |  2.91741705\n",
            "Step   8410: eval          Accuracy |  0.11560693\n",
            "\n",
            "Step   8420: Ran 10 train steps in 23.29 secs\n",
            "Step   8420: train CrossEntropyLoss |  2.85565996\n",
            "Step   8420: eval  CrossEntropyLoss |  2.99774647\n",
            "Step   8420: eval          Accuracy |  0.05309734\n",
            "\n",
            "Step   8430: Ran 10 train steps in 20.73 secs\n",
            "Step   8430: train CrossEntropyLoss |  2.87774777\n",
            "Step   8430: eval  CrossEntropyLoss |  2.83085918\n",
            "Step   8430: eval          Accuracy |  0.10810811\n",
            "\n",
            "Step   8440: Ran 10 train steps in 22.47 secs\n",
            "Step   8440: train CrossEntropyLoss |  2.70053458\n",
            "Step   8440: eval  CrossEntropyLoss |  2.91298723\n",
            "Step   8440: eval          Accuracy |  0.08648649\n",
            "\n",
            "Step   8450: Ran 10 train steps in 24.46 secs\n",
            "Step   8450: train CrossEntropyLoss |  2.84073782\n",
            "Step   8450: eval  CrossEntropyLoss |  2.69636488\n",
            "Step   8450: eval          Accuracy |  0.08235294\n",
            "\n",
            "Step   8460: Ran 10 train steps in 24.43 secs\n",
            "Step   8460: train CrossEntropyLoss |  2.89544582\n",
            "Step   8460: eval  CrossEntropyLoss |  3.14097142\n",
            "Step   8460: eval          Accuracy |  0.04347826\n",
            "\n",
            "Step   8470: Ran 10 train steps in 22.74 secs\n",
            "Step   8470: train CrossEntropyLoss |  2.77016020\n",
            "Step   8470: eval  CrossEntropyLoss |  2.30309963\n",
            "Step   8470: eval          Accuracy |  0.09917355\n",
            "\n",
            "Step   8480: Ran 10 train steps in 22.70 secs\n",
            "Step   8480: train CrossEntropyLoss |  2.74432516\n",
            "Step   8480: eval  CrossEntropyLoss |  2.69083309\n",
            "Step   8480: eval          Accuracy |  0.04166667\n",
            "\n",
            "Step   8490: Ran 10 train steps in 21.55 secs\n",
            "Step   8490: train CrossEntropyLoss |  2.81429935\n",
            "Step   8490: eval  CrossEntropyLoss |  2.66028404\n",
            "Step   8490: eval          Accuracy |  0.10404624\n",
            "\n",
            "Step   8500: Ran 10 train steps in 23.90 secs\n",
            "Step   8500: train CrossEntropyLoss |  2.78703380\n",
            "Step   8500: eval  CrossEntropyLoss |  2.63913441\n",
            "Step   8500: eval          Accuracy |  0.11702128\n",
            "\n",
            "Step   8510: Ran 10 train steps in 22.03 secs\n",
            "Step   8510: train CrossEntropyLoss |  2.88251543\n",
            "Step   8510: eval  CrossEntropyLoss |  3.10736752\n",
            "Step   8510: eval          Accuracy |  0.12230216\n",
            "\n",
            "Step   8520: Ran 10 train steps in 26.27 secs\n",
            "Step   8520: train CrossEntropyLoss |  2.91202259\n",
            "Step   8520: eval  CrossEntropyLoss |  2.94222188\n",
            "Step   8520: eval          Accuracy |  0.08225108\n",
            "\n",
            "Step   8530: Ran 10 train steps in 25.03 secs\n",
            "Step   8530: train CrossEntropyLoss |  2.80512571\n",
            "Step   8530: eval  CrossEntropyLoss |  2.77414107\n",
            "Step   8530: eval          Accuracy |  0.11242604\n",
            "\n",
            "Step   8540: Ran 10 train steps in 21.93 secs\n",
            "Step   8540: train CrossEntropyLoss |  2.85252047\n",
            "Step   8540: eval  CrossEntropyLoss |  3.08645225\n",
            "Step   8540: eval          Accuracy |  0.08391608\n",
            "\n",
            "Step   8550: Ran 10 train steps in 22.77 secs\n",
            "Step   8550: train CrossEntropyLoss |  2.85017157\n",
            "Step   8550: eval  CrossEntropyLoss |  3.02842736\n",
            "Step   8550: eval          Accuracy |  0.14173229\n",
            "\n",
            "Step   8560: Ran 10 train steps in 25.00 secs\n",
            "Step   8560: train CrossEntropyLoss |  2.61343527\n",
            "Step   8560: eval  CrossEntropyLoss |  2.70153022\n",
            "Step   8560: eval          Accuracy |  0.09392265\n",
            "\n",
            "Step   8570: Ran 10 train steps in 21.91 secs\n",
            "Step   8570: train CrossEntropyLoss |  2.75695992\n",
            "Step   8570: eval  CrossEntropyLoss |  2.56193447\n",
            "Step   8570: eval          Accuracy |  0.07812500\n",
            "\n",
            "Step   8580: Ran 10 train steps in 22.72 secs\n",
            "Step   8580: train CrossEntropyLoss |  2.76717615\n",
            "Step   8580: eval  CrossEntropyLoss |  2.29350877\n",
            "Step   8580: eval          Accuracy |  0.09459460\n",
            "\n",
            "Step   8590: Ran 10 train steps in 21.98 secs\n",
            "Step   8590: train CrossEntropyLoss |  2.87048483\n",
            "Step   8590: eval  CrossEntropyLoss |  2.38687015\n",
            "Step   8590: eval          Accuracy |  0.08433735\n",
            "\n",
            "Step   8600: Ran 10 train steps in 21.59 secs\n",
            "Step   8600: train CrossEntropyLoss |  2.77951360\n",
            "Step   8600: eval  CrossEntropyLoss |  3.13660932\n",
            "Step   8600: eval          Accuracy |  0.08441558\n",
            "\n",
            "Step   8610: Ran 10 train steps in 25.08 secs\n",
            "Step   8610: train CrossEntropyLoss |  2.91988254\n",
            "Step   8610: eval  CrossEntropyLoss |  3.43883729\n",
            "Step   8610: eval          Accuracy |  0.09420290\n",
            "\n",
            "Step   8620: Ran 10 train steps in 23.79 secs\n",
            "Step   8620: train CrossEntropyLoss |  2.91225863\n",
            "Step   8620: eval  CrossEntropyLoss |  2.29081988\n",
            "Step   8620: eval          Accuracy |  0.10112359\n",
            "\n",
            "Step   8630: Ran 10 train steps in 23.25 secs\n",
            "Step   8630: train CrossEntropyLoss |  2.74637699\n",
            "Step   8630: eval  CrossEntropyLoss |  2.48472452\n",
            "Step   8630: eval          Accuracy |  0.11111111\n",
            "\n",
            "Step   8640: Ran 10 train steps in 26.29 secs\n",
            "Step   8640: train CrossEntropyLoss |  2.69865298\n",
            "Step   8640: eval  CrossEntropyLoss |  2.77089357\n",
            "Step   8640: eval          Accuracy |  0.08641975\n",
            "\n",
            "Step   8650: Ran 10 train steps in 21.60 secs\n",
            "Step   8650: train CrossEntropyLoss |  2.98496795\n",
            "Step   8650: eval  CrossEntropyLoss |  2.44972396\n",
            "Step   8650: eval          Accuracy |  0.08510638\n",
            "\n",
            "Step   8660: Ran 10 train steps in 23.50 secs\n",
            "Step   8660: train CrossEntropyLoss |  2.59311461\n",
            "Step   8660: eval  CrossEntropyLoss |  2.20518255\n",
            "Step   8660: eval          Accuracy |  0.08510638\n",
            "\n",
            "Step   8670: Ran 10 train steps in 24.40 secs\n",
            "Step   8670: train CrossEntropyLoss |  2.77572584\n",
            "Step   8670: eval  CrossEntropyLoss |  2.91455579\n",
            "Step   8670: eval          Accuracy |  0.13793103\n",
            "\n",
            "Step   8680: Ran 10 train steps in 28.25 secs\n",
            "Step   8680: train CrossEntropyLoss |  2.79557228\n",
            "Step   8680: eval  CrossEntropyLoss |  3.15442944\n",
            "Step   8680: eval          Accuracy |  0.07812500\n",
            "\n",
            "Step   8690: Ran 10 train steps in 25.06 secs\n",
            "Step   8690: train CrossEntropyLoss |  2.86321497\n",
            "Step   8690: eval  CrossEntropyLoss |  2.89251518\n",
            "Step   8690: eval          Accuracy |  0.13157895\n",
            "\n",
            "Step   8700: Ran 10 train steps in 25.02 secs\n",
            "Step   8700: train CrossEntropyLoss |  2.63048172\n",
            "Step   8700: eval  CrossEntropyLoss |  2.84759331\n",
            "Step   8700: eval          Accuracy |  0.10891089\n",
            "\n",
            "Step   8710: Ran 10 train steps in 20.85 secs\n",
            "Step   8710: train CrossEntropyLoss |  2.84182405\n",
            "Step   8710: eval  CrossEntropyLoss |  2.75175428\n",
            "Step   8710: eval          Accuracy |  0.10759494\n",
            "\n",
            "Step   8720: Ran 10 train steps in 24.99 secs\n",
            "Step   8720: train CrossEntropyLoss |  2.73060942\n",
            "Step   8720: eval  CrossEntropyLoss |  2.69587278\n",
            "Step   8720: eval          Accuracy |  0.08241758\n",
            "\n",
            "Step   8730: Ran 10 train steps in 23.88 secs\n",
            "Step   8730: train CrossEntropyLoss |  2.80350280\n",
            "Step   8730: eval  CrossEntropyLoss |  2.73862839\n",
            "Step   8730: eval          Accuracy |  0.13333334\n",
            "\n",
            "Step   8740: Ran 10 train steps in 23.47 secs\n",
            "Step   8740: train CrossEntropyLoss |  2.89431572\n",
            "Step   8740: eval  CrossEntropyLoss |  3.22134757\n",
            "Step   8740: eval          Accuracy |  0.12500000\n",
            "\n",
            "Step   8750: Ran 10 train steps in 21.65 secs\n",
            "Step   8750: train CrossEntropyLoss |  2.48884988\n",
            "Step   8750: eval  CrossEntropyLoss |  2.56169987\n",
            "Step   8750: eval          Accuracy |  0.06779661\n",
            "\n",
            "Step   8760: Ran 10 train steps in 27.90 secs\n",
            "Step   8760: train CrossEntropyLoss |  2.84225488\n",
            "Step   8760: eval  CrossEntropyLoss |  2.54292464\n",
            "Step   8760: eval          Accuracy |  0.09016393\n",
            "\n",
            "Step   8770: Ran 10 train steps in 20.15 secs\n",
            "Step   8770: train CrossEntropyLoss |  2.84145451\n",
            "Step   8770: eval  CrossEntropyLoss |  3.21575141\n",
            "Step   8770: eval          Accuracy |  0.07801419\n",
            "\n",
            "Step   8780: Ran 10 train steps in 24.10 secs\n",
            "Step   8780: train CrossEntropyLoss |  2.73852658\n",
            "Step   8780: eval  CrossEntropyLoss |  2.47591519\n",
            "Step   8780: eval          Accuracy |  0.14285715\n",
            "\n",
            "Step   8790: Ran 10 train steps in 22.92 secs\n",
            "Step   8790: train CrossEntropyLoss |  2.95488477\n",
            "Step   8790: eval  CrossEntropyLoss |  3.00972652\n",
            "Step   8790: eval          Accuracy |  0.13333334\n",
            "\n",
            "Step   8800: Ran 10 train steps in 23.84 secs\n",
            "Step   8800: train CrossEntropyLoss |  2.94574022\n",
            "Step   8800: eval  CrossEntropyLoss |  2.23721552\n",
            "Step   8800: eval          Accuracy |  0.08510638\n",
            "\n",
            "Step   8810: Ran 10 train steps in 26.33 secs\n",
            "Step   8810: train CrossEntropyLoss |  2.78208661\n",
            "Step   8810: eval  CrossEntropyLoss |  2.57794809\n",
            "Step   8810: eval          Accuracy |  0.10679612\n",
            "\n",
            "Step   8820: Ran 10 train steps in 20.78 secs\n",
            "Step   8820: train CrossEntropyLoss |  2.76674390\n",
            "Step   8820: eval  CrossEntropyLoss |  2.34936261\n",
            "Step   8820: eval          Accuracy |  0.06593407\n",
            "\n",
            "Step   8830: Ran 10 train steps in 23.81 secs\n",
            "Step   8830: train CrossEntropyLoss |  2.76970100\n",
            "Step   8830: eval  CrossEntropyLoss |  2.63200569\n",
            "Step   8830: eval          Accuracy |  0.08695652\n",
            "\n",
            "Step   8840: Ran 10 train steps in 20.89 secs\n",
            "Step   8840: train CrossEntropyLoss |  2.88556862\n",
            "Step   8840: eval  CrossEntropyLoss |  3.46008015\n",
            "Step   8840: eval          Accuracy |  0.08648649\n",
            "\n",
            "Step   8850: Ran 10 train steps in 24.02 secs\n",
            "Step   8850: train CrossEntropyLoss |  2.67811966\n",
            "Step   8850: eval  CrossEntropyLoss |  3.32873940\n",
            "Step   8850: eval          Accuracy |  0.13333334\n",
            "\n",
            "Step   8860: Ran 10 train steps in 20.98 secs\n",
            "Step   8860: train CrossEntropyLoss |  2.76312852\n",
            "Step   8860: eval  CrossEntropyLoss |  2.80996561\n",
            "Step   8860: eval          Accuracy |  0.10625000\n",
            "\n",
            "Step   8870: Ran 10 train steps in 23.83 secs\n",
            "Step   8870: train CrossEntropyLoss |  2.71203136\n",
            "Step   8870: eval  CrossEntropyLoss |  2.46132541\n",
            "Step   8870: eval          Accuracy |  0.13157895\n",
            "\n",
            "Step   8880: Ran 10 train steps in 22.08 secs\n",
            "Step   8880: train CrossEntropyLoss |  2.78372097\n",
            "Step   8880: eval  CrossEntropyLoss |  2.51704383\n",
            "Step   8880: eval          Accuracy |  0.10204082\n",
            "\n",
            "Step   8890: Ran 10 train steps in 24.04 secs\n",
            "Step   8890: train CrossEntropyLoss |  2.77432060\n",
            "Step   8890: eval  CrossEntropyLoss |  2.80995846\n",
            "Step   8890: eval          Accuracy |  0.08641975\n",
            "\n",
            "Step   8900: Ran 10 train steps in 20.95 secs\n",
            "Step   8900: train CrossEntropyLoss |  2.84056520\n",
            "Step   8900: eval  CrossEntropyLoss |  2.63815904\n",
            "Step   8900: eval          Accuracy |  0.08029197\n",
            "\n",
            "Step   8910: Ran 10 train steps in 23.10 secs\n",
            "Step   8910: train CrossEntropyLoss |  2.83802915\n",
            "Step   8910: eval  CrossEntropyLoss |  3.13497901\n",
            "Step   8910: eval          Accuracy |  0.12598425\n",
            "\n",
            "Step   8920: Ran 10 train steps in 26.28 secs\n",
            "Step   8920: train CrossEntropyLoss |  2.78292513\n",
            "Step   8920: eval  CrossEntropyLoss |  3.40354967\n",
            "Step   8920: eval          Accuracy |  0.05000000\n",
            "\n",
            "Step   8930: Ran 10 train steps in 26.16 secs\n",
            "Step   8930: train CrossEntropyLoss |  2.74006557\n",
            "Step   8930: eval  CrossEntropyLoss |  2.55507183\n",
            "Step   8930: eval          Accuracy |  0.11290322\n",
            "\n",
            "Step   8940: Ran 10 train steps in 21.49 secs\n",
            "Step   8940: train CrossEntropyLoss |  2.87605476\n",
            "Step   8940: eval  CrossEntropyLoss |  2.55623984\n",
            "Step   8940: eval          Accuracy |  0.13147411\n",
            "\n",
            "Step   8950: Ran 10 train steps in 19.07 secs\n",
            "Step   8950: train CrossEntropyLoss |  2.79833555\n",
            "Step   8950: eval  CrossEntropyLoss |  3.31437778\n",
            "Step   8950: eval          Accuracy |  0.07142857\n",
            "\n",
            "Step   8960: Ran 10 train steps in 22.81 secs\n",
            "Step   8960: train CrossEntropyLoss |  2.72341299\n",
            "Step   8960: eval  CrossEntropyLoss |  3.11291051\n",
            "Step   8960: eval          Accuracy |  0.09756097\n",
            "\n",
            "Step   8970: Ran 10 train steps in 26.62 secs\n",
            "Step   8970: train CrossEntropyLoss |  2.80264592\n",
            "Step   8970: eval  CrossEntropyLoss |  2.94830465\n",
            "Step   8970: eval          Accuracy |  0.11940298\n",
            "\n",
            "Step   8980: Ran 10 train steps in 23.49 secs\n",
            "Step   8980: train CrossEntropyLoss |  2.56443667\n",
            "Step   8980: eval  CrossEntropyLoss |  2.79079032\n",
            "Step   8980: eval          Accuracy |  0.14473684\n",
            "\n",
            "Step   8990: Ran 10 train steps in 24.02 secs\n",
            "Step   8990: train CrossEntropyLoss |  2.68897843\n",
            "Step   8990: eval  CrossEntropyLoss |  2.07236147\n",
            "Step   8990: eval          Accuracy |  0.17021276\n",
            "\n",
            "Step   9000: Ran 10 train steps in 23.32 secs\n",
            "Step   9000: train CrossEntropyLoss |  2.84996748\n",
            "Step   9000: eval  CrossEntropyLoss |  2.48206234\n",
            "Step   9000: eval          Accuracy |  0.07812500\n",
            "\n",
            "Step   9010: Ran 10 train steps in 26.97 secs\n",
            "Step   9010: train CrossEntropyLoss |  2.70725870\n",
            "Step   9010: eval  CrossEntropyLoss |  2.83078456\n",
            "Step   9010: eval          Accuracy |  0.10526316\n",
            "\n",
            "Step   9020: Ran 10 train steps in 23.27 secs\n",
            "Step   9020: train CrossEntropyLoss |  3.00564146\n",
            "Step   9020: eval  CrossEntropyLoss |  2.45091915\n",
            "Step   9020: eval          Accuracy |  0.06756756\n",
            "\n",
            "Step   9030: Ran 10 train steps in 23.33 secs\n",
            "Step   9030: train CrossEntropyLoss |  2.67454147\n",
            "Step   9030: eval  CrossEntropyLoss |  2.90914559\n",
            "Step   9030: eval          Accuracy |  0.08235294\n",
            "\n",
            "Step   9040: Ran 10 train steps in 22.99 secs\n",
            "Step   9040: train CrossEntropyLoss |  2.88281155\n",
            "Step   9040: eval  CrossEntropyLoss |  2.38701272\n",
            "Step   9040: eval          Accuracy |  0.08176101\n",
            "\n",
            "Step   9050: Ran 10 train steps in 23.38 secs\n",
            "Step   9050: train CrossEntropyLoss |  2.60247087\n",
            "Step   9050: eval  CrossEntropyLoss |  2.21140456\n",
            "Step   9050: eval          Accuracy |  0.09302326\n",
            "\n",
            "Step   9060: Ran 10 train steps in 26.18 secs\n",
            "Step   9060: train CrossEntropyLoss |  2.90494633\n",
            "Step   9060: eval  CrossEntropyLoss |  2.89033604\n",
            "Step   9060: eval          Accuracy |  0.08571429\n",
            "\n",
            "Step   9070: Ran 10 train steps in 21.84 secs\n",
            "Step   9070: train CrossEntropyLoss |  2.81404257\n",
            "Step   9070: eval  CrossEntropyLoss |  2.89792991\n",
            "Step   9070: eval          Accuracy |  0.04347826\n",
            "\n",
            "Step   9080: Ran 10 train steps in 24.39 secs\n",
            "Step   9080: train CrossEntropyLoss |  2.87417603\n",
            "Step   9080: eval  CrossEntropyLoss |  2.35150099\n",
            "Step   9080: eval          Accuracy |  0.10526316\n",
            "\n",
            "Step   9090: Ran 10 train steps in 22.33 secs\n",
            "Step   9090: train CrossEntropyLoss |  2.76651335\n",
            "Step   9090: eval  CrossEntropyLoss |  3.08900380\n",
            "Step   9090: eval          Accuracy |  0.10416666\n",
            "\n",
            "Step   9100: Ran 10 train steps in 21.78 secs\n",
            "Step   9100: train CrossEntropyLoss |  2.80576396\n",
            "Step   9100: eval  CrossEntropyLoss |  2.65821314\n",
            "Step   9100: eval          Accuracy |  0.08783784\n",
            "\n",
            "Step   9110: Ran 10 train steps in 22.79 secs\n",
            "Step   9110: train CrossEntropyLoss |  2.95286679\n",
            "Step   9110: eval  CrossEntropyLoss |  2.84160233\n",
            "Step   9110: eval          Accuracy |  0.10240964\n",
            "\n",
            "Step   9120: Ran 10 train steps in 25.38 secs\n",
            "Step   9120: train CrossEntropyLoss |  2.75558352\n",
            "Step   9120: eval  CrossEntropyLoss |  2.39127946\n",
            "Step   9120: eval          Accuracy |  0.06250000\n",
            "\n",
            "Step   9130: Ran 10 train steps in 22.28 secs\n",
            "Step   9130: train CrossEntropyLoss |  2.71865606\n",
            "Step   9130: eval  CrossEntropyLoss |  2.70172977\n",
            "Step   9130: eval          Accuracy |  0.18518518\n",
            "\n",
            "Step   9140: Ran 10 train steps in 22.86 secs\n",
            "Step   9140: train CrossEntropyLoss |  2.88619471\n",
            "Step   9140: eval  CrossEntropyLoss |  2.86644840\n",
            "Step   9140: eval          Accuracy |  0.04615385\n",
            "\n",
            "Step   9150: Ran 10 train steps in 23.79 secs\n",
            "Step   9150: train CrossEntropyLoss |  2.77473116\n",
            "Step   9150: eval  CrossEntropyLoss |  2.55735064\n",
            "Step   9150: eval          Accuracy |  0.11206897\n",
            "\n",
            "Step   9160: Ran 10 train steps in 22.13 secs\n",
            "Step   9160: train CrossEntropyLoss |  2.72270536\n",
            "Step   9160: eval  CrossEntropyLoss |  2.19181752\n",
            "Step   9160: eval          Accuracy |  0.07894737\n",
            "\n",
            "Step   9170: Ran 10 train steps in 24.40 secs\n",
            "Step   9170: train CrossEntropyLoss |  2.84864855\n",
            "Step   9170: eval  CrossEntropyLoss |  2.47131467\n",
            "Step   9170: eval          Accuracy |  0.08264463\n",
            "\n",
            "Step   9180: Ran 10 train steps in 22.85 secs\n",
            "Step   9180: train CrossEntropyLoss |  3.01451683\n",
            "Step   9180: eval  CrossEntropyLoss |  2.65043902\n",
            "Step   9180: eval          Accuracy |  0.10734463\n",
            "\n",
            "Step   9190: Ran 10 train steps in 20.91 secs\n",
            "Step   9190: train CrossEntropyLoss |  2.81435084\n",
            "Step   9190: eval  CrossEntropyLoss |  2.90382481\n",
            "Step   9190: eval          Accuracy |  0.08695652\n",
            "\n",
            "Step   9200: Ran 10 train steps in 23.84 secs\n",
            "Step   9200: train CrossEntropyLoss |  2.75986719\n",
            "Step   9200: eval  CrossEntropyLoss |  2.93845510\n",
            "Step   9200: eval          Accuracy |  0.08196721\n",
            "\n",
            "Step   9210: Ran 10 train steps in 22.81 secs\n",
            "Step   9210: train CrossEntropyLoss |  2.94749022\n",
            "Step   9210: eval  CrossEntropyLoss |  2.62050939\n",
            "Step   9210: eval          Accuracy |  0.10465116\n",
            "\n",
            "Step   9220: Ran 10 train steps in 21.42 secs\n",
            "Step   9220: train CrossEntropyLoss |  2.67903924\n",
            "Step   9220: eval  CrossEntropyLoss |  2.56301880\n",
            "Step   9220: eval          Accuracy |  0.10951009\n",
            "\n",
            "Step   9230: Ran 10 train steps in 25.76 secs\n",
            "Step   9230: train CrossEntropyLoss |  2.75198984\n",
            "Step   9230: eval  CrossEntropyLoss |  3.18938160\n",
            "Step   9230: eval          Accuracy |  0.05000000\n",
            "\n",
            "Step   9240: Ran 10 train steps in 27.39 secs\n",
            "Step   9240: train CrossEntropyLoss |  2.89161038\n",
            "Step   9240: eval  CrossEntropyLoss |  3.27270031\n",
            "Step   9240: eval          Accuracy |  0.09708738\n",
            "\n",
            "Step   9250: Ran 10 train steps in 23.36 secs\n",
            "Step   9250: train CrossEntropyLoss |  2.75158215\n",
            "Step   9250: eval  CrossEntropyLoss |  2.79126883\n",
            "Step   9250: eval          Accuracy |  0.09933775\n",
            "\n",
            "Step   9260: Ran 10 train steps in 22.19 secs\n",
            "Step   9260: train CrossEntropyLoss |  2.76005101\n",
            "Step   9260: eval  CrossEntropyLoss |  2.77071524\n",
            "Step   9260: eval          Accuracy |  0.12582782\n",
            "\n",
            "Step   9270: Ran 10 train steps in 26.31 secs\n",
            "Step   9270: train CrossEntropyLoss |  2.70027256\n",
            "Step   9270: eval  CrossEntropyLoss |  3.15699077\n",
            "Step   9270: eval          Accuracy |  0.11042945\n",
            "\n",
            "Step   9280: Ran 10 train steps in 21.04 secs\n",
            "Step   9280: train CrossEntropyLoss |  2.71049738\n",
            "Step   9280: eval  CrossEntropyLoss |  2.52440238\n",
            "Step   9280: eval          Accuracy |  0.15000001\n",
            "\n",
            "Step   9290: Ran 10 train steps in 22.27 secs\n",
            "Step   9290: train CrossEntropyLoss |  2.98804379\n",
            "Step   9290: eval  CrossEntropyLoss |  2.91673470\n",
            "Step   9290: eval          Accuracy |  0.09375000\n",
            "\n",
            "Step   9300: Ran 10 train steps in 22.91 secs\n",
            "Step   9300: train CrossEntropyLoss |  2.87704349\n",
            "Step   9300: eval  CrossEntropyLoss |  3.01957202\n",
            "Step   9300: eval          Accuracy |  0.15023474\n",
            "\n",
            "Step   9310: Ran 10 train steps in 23.89 secs\n",
            "Step   9310: train CrossEntropyLoss |  2.61437559\n",
            "Step   9310: eval  CrossEntropyLoss |  3.50702333\n",
            "Step   9310: eval          Accuracy |  0.11965812\n",
            "\n",
            "Step   9320: Ran 10 train steps in 21.63 secs\n",
            "Step   9320: train CrossEntropyLoss |  3.02649045\n",
            "Step   9320: eval  CrossEntropyLoss |  2.63560605\n",
            "Step   9320: eval          Accuracy |  0.12500000\n",
            "\n",
            "Step   9330: Ran 10 train steps in 25.69 secs\n",
            "Step   9330: train CrossEntropyLoss |  2.77731657\n",
            "Step   9330: eval  CrossEntropyLoss |  2.56425714\n",
            "Step   9330: eval          Accuracy |  0.06578948\n",
            "\n",
            "Step   9340: Ran 10 train steps in 22.84 secs\n",
            "Step   9340: train CrossEntropyLoss |  2.90448022\n",
            "Step   9340: eval  CrossEntropyLoss |  2.49866152\n",
            "Step   9340: eval          Accuracy |  0.08333334\n",
            "\n",
            "Step   9350: Ran 10 train steps in 24.68 secs\n",
            "Step   9350: train CrossEntropyLoss |  2.68853259\n",
            "Step   9350: eval  CrossEntropyLoss |  2.80530548\n",
            "Step   9350: eval          Accuracy |  0.14341085\n",
            "\n",
            "Step   9360: Ran 10 train steps in 28.62 secs\n",
            "Step   9360: train CrossEntropyLoss |  2.67172241\n",
            "Step   9360: eval  CrossEntropyLoss |  3.07534051\n",
            "Step   9360: eval          Accuracy |  0.13076924\n",
            "\n",
            "Step   9370: Ran 10 train steps in 22.31 secs\n",
            "Step   9370: train CrossEntropyLoss |  2.88345146\n",
            "Step   9370: eval  CrossEntropyLoss |  2.85769677\n",
            "Step   9370: eval          Accuracy |  0.09142857\n",
            "\n",
            "Step   9380: Ran 10 train steps in 23.05 secs\n",
            "Step   9380: train CrossEntropyLoss |  2.78743410\n",
            "Step   9380: eval  CrossEntropyLoss |  3.45815349\n",
            "Step   9380: eval          Accuracy |  0.09900990\n",
            "\n",
            "Step   9390: Ran 10 train steps in 21.82 secs\n",
            "Step   9390: train CrossEntropyLoss |  2.91098166\n",
            "Step   9390: eval  CrossEntropyLoss |  2.74943066\n",
            "Step   9390: eval          Accuracy |  0.09285714\n",
            "\n",
            "Step   9400: Ran 10 train steps in 23.32 secs\n",
            "Step   9400: train CrossEntropyLoss |  2.88296461\n",
            "Step   9400: eval  CrossEntropyLoss |  2.46736503\n",
            "Step   9400: eval          Accuracy |  0.09578544\n",
            "\n",
            "Step   9410: Ran 10 train steps in 23.45 secs\n",
            "Step   9410: train CrossEntropyLoss |  2.90695238\n",
            "Step   9410: eval  CrossEntropyLoss |  2.76936984\n",
            "Step   9410: eval          Accuracy |  0.11458334\n",
            "\n",
            "Step   9420: Ran 10 train steps in 21.79 secs\n",
            "Step   9420: train CrossEntropyLoss |  3.05248499\n",
            "Step   9420: eval  CrossEntropyLoss |  2.71656775\n",
            "Step   9420: eval          Accuracy |  0.09589041\n",
            "\n",
            "Step   9430: Ran 10 train steps in 23.41 secs\n",
            "Step   9430: train CrossEntropyLoss |  3.00478053\n",
            "Step   9430: eval  CrossEntropyLoss |  3.22905707\n",
            "Step   9430: eval          Accuracy |  0.10132159\n",
            "\n",
            "Step   9440: Ran 10 train steps in 22.30 secs\n",
            "Step   9440: train CrossEntropyLoss |  2.69894457\n",
            "Step   9440: eval  CrossEntropyLoss |  3.07810831\n",
            "Step   9440: eval          Accuracy |  0.12621360\n",
            "\n",
            "Step   9450: Ran 10 train steps in 23.91 secs\n",
            "Step   9450: train CrossEntropyLoss |  2.75308490\n",
            "Step   9450: eval  CrossEntropyLoss |  2.83615494\n",
            "Step   9450: eval          Accuracy |  0.11340206\n",
            "\n",
            "Step   9460: Ran 10 train steps in 21.94 secs\n",
            "Step   9460: train CrossEntropyLoss |  2.68513536\n",
            "Step   9460: eval  CrossEntropyLoss |  2.85544705\n",
            "Step   9460: eval          Accuracy |  0.06521739\n",
            "\n",
            "Step   9470: Ran 10 train steps in 27.37 secs\n",
            "Step   9470: train CrossEntropyLoss |  2.82766819\n",
            "Step   9470: eval  CrossEntropyLoss |  3.15752625\n",
            "Step   9470: eval          Accuracy |  0.09569378\n",
            "\n",
            "Step   9480: Ran 10 train steps in 21.11 secs\n",
            "Step   9480: train CrossEntropyLoss |  2.71114755\n",
            "Step   9480: eval  CrossEntropyLoss |  2.26196003\n",
            "Step   9480: eval          Accuracy |  0.07272727\n",
            "\n",
            "Step   9490: Ran 10 train steps in 22.82 secs\n",
            "Step   9490: train CrossEntropyLoss |  2.72317195\n",
            "Step   9490: eval  CrossEntropyLoss |  2.47134590\n",
            "Step   9490: eval          Accuracy |  0.08888889\n",
            "\n",
            "Step   9500: Ran 10 train steps in 20.61 secs\n",
            "Step   9500: train CrossEntropyLoss |  2.74377751\n",
            "Step   9500: eval  CrossEntropyLoss |  3.08152461\n",
            "Step   9500: eval          Accuracy |  0.10687023\n",
            "\n",
            "Step   9510: Ran 10 train steps in 25.20 secs\n",
            "Step   9510: train CrossEntropyLoss |  3.03363204\n",
            "Step   9510: eval  CrossEntropyLoss |  2.81472850\n",
            "Step   9510: eval          Accuracy |  0.04255319\n",
            "\n",
            "Step   9520: Ran 10 train steps in 25.81 secs\n",
            "Step   9520: train CrossEntropyLoss |  2.91007853\n",
            "Step   9520: eval  CrossEntropyLoss |  2.96965861\n",
            "Step   9520: eval          Accuracy |  0.11612903\n",
            "\n",
            "Step   9530: Ran 10 train steps in 25.35 secs\n",
            "Step   9530: train CrossEntropyLoss |  2.90730858\n",
            "Step   9530: eval  CrossEntropyLoss |  2.29096937\n",
            "Step   9530: eval          Accuracy |  0.08064516\n",
            "\n",
            "Step   9540: Ran 10 train steps in 24.56 secs\n",
            "Step   9540: train CrossEntropyLoss |  2.64943075\n",
            "Step   9540: eval  CrossEntropyLoss |  3.24631739\n",
            "Step   9540: eval          Accuracy |  0.10526316\n",
            "\n",
            "Step   9550: Ran 10 train steps in 25.18 secs\n",
            "Step   9550: train CrossEntropyLoss |  2.87081480\n",
            "Step   9550: eval  CrossEntropyLoss |  2.58152652\n",
            "Step   9550: eval          Accuracy |  0.08450704\n",
            "\n",
            "Step   9560: Ran 10 train steps in 24.76 secs\n",
            "Step   9560: train CrossEntropyLoss |  2.74991465\n",
            "Step   9560: eval  CrossEntropyLoss |  2.81229663\n",
            "Step   9560: eval          Accuracy |  0.10526316\n",
            "\n",
            "Step   9570: Ran 10 train steps in 19.03 secs\n",
            "Step   9570: train CrossEntropyLoss |  2.83638620\n",
            "Step   9570: eval  CrossEntropyLoss |  2.44000554\n",
            "Step   9570: eval          Accuracy |  0.11475410\n",
            "\n",
            "Step   9580: Ran 10 train steps in 26.88 secs\n",
            "Step   9580: train CrossEntropyLoss |  2.77846098\n",
            "Step   9580: eval  CrossEntropyLoss |  2.68085647\n",
            "Step   9580: eval          Accuracy |  0.12820514\n",
            "\n",
            "Step   9590: Ran 10 train steps in 17.94 secs\n",
            "Step   9590: train CrossEntropyLoss |  2.71804523\n",
            "Step   9590: eval  CrossEntropyLoss |  3.18599486\n",
            "Step   9590: eval          Accuracy |  0.08403362\n",
            "\n",
            "Step   9600: Ran 10 train steps in 22.81 secs\n",
            "Step   9600: train CrossEntropyLoss |  2.91466951\n",
            "Step   9600: eval  CrossEntropyLoss |  2.60644627\n",
            "Step   9600: eval          Accuracy |  0.05263158\n",
            "\n",
            "Step   9610: Ran 10 train steps in 21.66 secs\n",
            "Step   9610: train CrossEntropyLoss |  2.77022409\n",
            "Step   9610: eval  CrossEntropyLoss |  2.93607712\n",
            "Step   9610: eval          Accuracy |  0.09039548\n",
            "\n",
            "Step   9620: Ran 10 train steps in 25.11 secs\n",
            "Step   9620: train CrossEntropyLoss |  2.72591090\n",
            "Step   9620: eval  CrossEntropyLoss |  2.61330342\n",
            "Step   9620: eval          Accuracy |  0.11250000\n",
            "\n",
            "Step   9630: Ran 10 train steps in 25.12 secs\n",
            "Step   9630: train CrossEntropyLoss |  2.65245533\n",
            "Step   9630: eval  CrossEntropyLoss |  2.96102309\n",
            "Step   9630: eval          Accuracy |  0.08256881\n",
            "\n",
            "Step   9640: Ran 10 train steps in 21.62 secs\n",
            "Step   9640: train CrossEntropyLoss |  2.86233449\n",
            "Step   9640: eval  CrossEntropyLoss |  3.24975657\n",
            "Step   9640: eval          Accuracy |  0.06250000\n",
            "\n",
            "Step   9650: Ran 10 train steps in 22.21 secs\n",
            "Step   9650: train CrossEntropyLoss |  2.95578742\n",
            "Step   9650: eval  CrossEntropyLoss |  3.19464779\n",
            "Step   9650: eval          Accuracy |  0.12280702\n",
            "\n",
            "Step   9660: Ran 10 train steps in 25.19 secs\n",
            "Step   9660: train CrossEntropyLoss |  2.88452768\n",
            "Step   9660: eval  CrossEntropyLoss |  2.78891945\n",
            "Step   9660: eval          Accuracy |  0.10738255\n",
            "\n",
            "Step   9670: Ran 10 train steps in 19.78 secs\n",
            "Step   9670: train CrossEntropyLoss |  2.99730372\n",
            "Step   9670: eval  CrossEntropyLoss |  2.68279457\n",
            "Step   9670: eval          Accuracy |  0.11023622\n",
            "\n",
            "Step   9680: Ran 10 train steps in 25.20 secs\n",
            "Step   9680: train CrossEntropyLoss |  2.94090557\n",
            "Step   9680: eval  CrossEntropyLoss |  2.49388289\n",
            "Step   9680: eval          Accuracy |  0.07692308\n",
            "\n",
            "Step   9690: Ran 10 train steps in 22.86 secs\n",
            "Step   9690: train CrossEntropyLoss |  2.75467300\n",
            "Step   9690: eval  CrossEntropyLoss |  2.54753232\n",
            "Step   9690: eval          Accuracy |  0.11464968\n",
            "\n",
            "Step   9700: Ran 10 train steps in 21.74 secs\n",
            "Step   9700: train CrossEntropyLoss |  2.69052362\n",
            "Step   9700: eval  CrossEntropyLoss |  2.79443622\n",
            "Step   9700: eval          Accuracy |  0.12000000\n",
            "\n",
            "Step   9710: Ran 10 train steps in 23.36 secs\n",
            "Step   9710: train CrossEntropyLoss |  2.87305403\n",
            "Step   9710: eval  CrossEntropyLoss |  3.57844496\n",
            "Step   9710: eval          Accuracy |  0.07894737\n",
            "\n",
            "Step   9720: Ran 10 train steps in 28.49 secs\n",
            "Step   9720: train CrossEntropyLoss |  2.92346954\n",
            "Step   9720: eval  CrossEntropyLoss |  2.60370874\n",
            "Step   9720: eval          Accuracy |  0.10191083\n",
            "\n",
            "Step   9730: Ran 10 train steps in 24.19 secs\n",
            "Step   9730: train CrossEntropyLoss |  2.75998068\n",
            "Step   9730: eval  CrossEntropyLoss |  3.20480847\n",
            "Step   9730: eval          Accuracy |  0.12258065\n",
            "\n",
            "Step   9740: Ran 10 train steps in 20.73 secs\n",
            "Step   9740: train CrossEntropyLoss |  2.64152813\n",
            "Step   9740: eval  CrossEntropyLoss |  1.86163008\n",
            "Step   9740: eval          Accuracy |  0.10169491\n",
            "\n",
            "Step   9750: Ran 10 train steps in 23.19 secs\n",
            "Step   9750: train CrossEntropyLoss |  2.87498188\n",
            "Step   9750: eval  CrossEntropyLoss |  2.49561548\n",
            "Step   9750: eval          Accuracy |  0.14146341\n",
            "\n",
            "Step   9760: Ran 10 train steps in 26.43 secs\n",
            "Step   9760: train CrossEntropyLoss |  2.92391205\n",
            "Step   9760: eval  CrossEntropyLoss |  2.73291731\n",
            "Step   9760: eval          Accuracy |  0.11250000\n",
            "\n",
            "Step   9770: Ran 10 train steps in 25.24 secs\n",
            "Step   9770: train CrossEntropyLoss |  2.64879847\n",
            "Step   9770: eval  CrossEntropyLoss |  2.87717223\n",
            "Step   9770: eval          Accuracy |  0.08982036\n",
            "\n",
            "Step   9780: Ran 10 train steps in 22.39 secs\n",
            "Step   9780: train CrossEntropyLoss |  2.60982370\n",
            "Step   9780: eval  CrossEntropyLoss |  2.41957808\n",
            "Step   9780: eval          Accuracy |  0.09722222\n",
            "\n",
            "Step   9790: Ran 10 train steps in 24.04 secs\n",
            "Step   9790: train CrossEntropyLoss |  2.77692842\n",
            "Step   9790: eval  CrossEntropyLoss |  2.78432465\n",
            "Step   9790: eval          Accuracy |  0.08461539\n",
            "\n",
            "Step   9800: Ran 10 train steps in 22.93 secs\n",
            "Step   9800: train CrossEntropyLoss |  2.99690604\n",
            "Step   9800: eval  CrossEntropyLoss |  2.37847090\n",
            "Step   9800: eval          Accuracy |  0.12109375\n",
            "\n",
            "Step   9810: Ran 10 train steps in 24.50 secs\n",
            "Step   9810: train CrossEntropyLoss |  2.95096064\n",
            "Step   9810: eval  CrossEntropyLoss |  2.64592695\n",
            "Step   9810: eval          Accuracy |  0.08148148\n",
            "\n",
            "Step   9820: Ran 10 train steps in 21.75 secs\n",
            "Step   9820: train CrossEntropyLoss |  2.64502668\n",
            "Step   9820: eval  CrossEntropyLoss |  2.29647088\n",
            "Step   9820: eval          Accuracy |  0.10714286\n",
            "\n",
            "Step   9830: Ran 10 train steps in 22.56 secs\n",
            "Step   9830: train CrossEntropyLoss |  2.63856435\n",
            "Step   9830: eval  CrossEntropyLoss |  3.28468394\n",
            "Step   9830: eval          Accuracy |  0.10924370\n",
            "\n",
            "Step   9840: Ran 10 train steps in 24.92 secs\n",
            "Step   9840: train CrossEntropyLoss |  2.80887270\n",
            "Step   9840: eval  CrossEntropyLoss |  3.31857634\n",
            "Step   9840: eval          Accuracy |  0.10810811\n",
            "\n",
            "Step   9850: Ran 10 train steps in 26.85 secs\n",
            "Step   9850: train CrossEntropyLoss |  2.94404578\n",
            "Step   9850: eval  CrossEntropyLoss |  2.77479291\n",
            "Step   9850: eval          Accuracy |  0.07111111\n",
            "\n",
            "Step   9860: Ran 10 train steps in 25.24 secs\n",
            "Step   9860: train CrossEntropyLoss |  2.91548347\n",
            "Step   9860: eval  CrossEntropyLoss |  2.40130234\n",
            "Step   9860: eval          Accuracy |  0.06756756\n",
            "\n",
            "Step   9870: Ran 10 train steps in 25.29 secs\n",
            "Step   9870: train CrossEntropyLoss |  2.75188255\n",
            "Step   9870: eval  CrossEntropyLoss |  2.41173649\n",
            "Step   9870: eval          Accuracy |  0.11602210\n",
            "\n",
            "Step   9880: Ran 10 train steps in 24.61 secs\n",
            "Step   9880: train CrossEntropyLoss |  2.76243258\n",
            "Step   9880: eval  CrossEntropyLoss |  2.67411137\n",
            "Step   9880: eval          Accuracy |  0.11363637\n",
            "\n",
            "Step   9890: Ran 10 train steps in 25.18 secs\n",
            "Step   9890: train CrossEntropyLoss |  2.73123813\n",
            "Step   9890: eval  CrossEntropyLoss |  2.49708724\n",
            "Step   9890: eval          Accuracy |  0.09933775\n",
            "\n",
            "Step   9900: Ran 10 train steps in 21.07 secs\n",
            "Step   9900: train CrossEntropyLoss |  2.72516584\n",
            "Step   9900: eval  CrossEntropyLoss |  2.67327642\n",
            "Step   9900: eval          Accuracy |  0.08474576\n",
            "\n",
            "Step   9910: Ran 10 train steps in 25.31 secs\n",
            "Step   9910: train CrossEntropyLoss |  2.83755922\n",
            "Step   9910: eval  CrossEntropyLoss |  2.94776082\n",
            "Step   9910: eval          Accuracy |  0.11146497\n",
            "\n",
            "Step   9920: Ran 10 train steps in 25.28 secs\n",
            "Step   9920: train CrossEntropyLoss |  2.84124112\n",
            "Step   9920: eval  CrossEntropyLoss |  2.36570740\n",
            "Step   9920: eval          Accuracy |  0.05617978\n",
            "\n",
            "Step   9930: Ran 10 train steps in 21.05 secs\n",
            "Step   9930: train CrossEntropyLoss |  2.66946387\n",
            "Step   9930: eval  CrossEntropyLoss |  2.99774361\n",
            "Step   9930: eval          Accuracy |  0.07751938\n",
            "\n",
            "Step   9940: Ran 10 train steps in 24.61 secs\n",
            "Step   9940: train CrossEntropyLoss |  2.79920220\n",
            "Step   9940: eval  CrossEntropyLoss |  2.51105165\n",
            "Step   9940: eval          Accuracy |  0.07594936\n",
            "\n",
            "Step   9950: Ran 10 train steps in 25.78 secs\n",
            "Step   9950: train CrossEntropyLoss |  2.82046866\n",
            "Step   9950: eval  CrossEntropyLoss |  2.50156498\n",
            "Step   9950: eval          Accuracy |  0.07518797\n",
            "\n",
            "Step   9960: Ran 10 train steps in 20.23 secs\n",
            "Step   9960: train CrossEntropyLoss |  2.82907820\n",
            "Step   9960: eval  CrossEntropyLoss |  2.79301524\n",
            "Step   9960: eval          Accuracy |  0.11111111\n",
            "\n",
            "Step   9970: Ran 10 train steps in 20.09 secs\n",
            "Step   9970: train CrossEntropyLoss |  2.84800005\n",
            "Step   9970: eval  CrossEntropyLoss |  2.54735923\n",
            "Step   9970: eval          Accuracy |  0.12041885\n",
            "\n",
            "Step   9980: Ran 10 train steps in 24.15 secs\n",
            "Step   9980: train CrossEntropyLoss |  2.74635172\n",
            "Step   9980: eval  CrossEntropyLoss |  3.57544494\n",
            "Step   9980: eval          Accuracy |  0.11111111\n",
            "\n",
            "Step   9990: Ran 10 train steps in 22.94 secs\n",
            "Step   9990: train CrossEntropyLoss |  2.83261538\n",
            "Step   9990: eval  CrossEntropyLoss |  2.67454123\n",
            "Step   9990: eval          Accuracy |  0.11594203\n",
            "\n",
            "Step  10000: Ran 10 train steps in 29.15 secs\n",
            "Step  10000: train CrossEntropyLoss |  2.75298619\n",
            "Step  10000: eval  CrossEntropyLoss |  2.43224692\n",
            "Step  10000: eval          Accuracy |  0.10552764\n",
            "\n",
            "Step  10010: Ran 10 train steps in 24.03 secs\n",
            "Step  10010: train CrossEntropyLoss |  2.54764533\n",
            "Step  10010: eval  CrossEntropyLoss |  2.31945753\n",
            "Step  10010: eval          Accuracy |  0.03947368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "6vAdls2FG1K1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKrEBjmskeWa"
      },
      "source": [
        "\n",
        " # Part 4:  Evaluation  \n",
        "\n",
        "### 4.1 Loading in a trained model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr2MpCrdToVu",
        "outputId": "b5bc1f82-270f-489f-c517-0d6c93915332"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['eval_model',\n",
              " 'eval_tasks',\n",
              " 'history',\n",
              " 'is_chief',\n",
              " 'load_checkpoint',\n",
              " 'log_summary',\n",
              " 'model',\n",
              " 'n_devices',\n",
              " 'new_rng',\n",
              " 'output_dir',\n",
              " 'run',\n",
              " 'run_evals',\n",
              " 'save_checkpoint',\n",
              " 'step',\n",
              " 'tasks',\n",
              " 'update_weights_and_state']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "[x for x in dir(loop) if not(str(x).startswith('_'))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohTV9-P0Tt_B"
      },
      "outputs": [],
      "source": [
        "model = loop.model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilM9C8P3RWRf"
      },
      "source": [
        "<a name='5'></a>\n",
        "# Part 5: Testing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rD_bXRCpRWRg"
      },
      "outputs": [],
      "source": [
        "def next_symbol(output_tokens, model):\n",
        "  token_length = len(output_tokens)\n",
        "  padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\n",
        "  padded = output_tokens + [0] * (padded_length - token_length)\n",
        "  padded_with_batch = np.array(padded)[None, :]\n",
        "  output, _ = model((padded_with_batch, padded_with_batch))\n",
        "  log_probs = output[0, token_length, :]\n",
        "\n",
        "  return int(np.argmax(log_probs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GA3iXCNooD1k",
        "outputId": "d58ef0a4-a46f-414c-8df9-1875e6cfb1c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "# Testing\n",
        "sentence_test_nxt_symbl = \"Before the times of GPS and cell phones, maps were the main way to find one's way from point A to point B without getting lost. Nowadays, we have maps for everything from land elevation to animal location origins. Maps are fantastic things, just ask the cartographer at the local map store, he'll be sure to feel the same way. So if you're ready to take a look at 40 of the most interesting maps of America, keep reading!\"\n",
        "detokenizer([next_symbol(tokenizer(sentence_test_nxt_symbl)+[0], model)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAgTVb-WK9vs"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AwrQFglRWRj"
      },
      "source": [
        "<a name='5.1'></a>\n",
        "### 5.1 Greedy decoding\n",
        "\n",
        "input_sentence & trained model -> decoded sentence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HwIdimiN0k2"
      },
      "outputs": [],
      "source": [
        "# Decoding functions.\n",
        "def greedy_decode(input_text, model, max_n = 100, show_progress = False):\n",
        "  # Using tokenizer()\n",
        "  processed_text = process_text(input_text)\n",
        "  cur_output_tokens = tokenizer(processed_text) + [0]\n",
        "  generated_output = []\n",
        "  cur_output = 0\n",
        "  EOS = 1\n",
        "  i = 0\n",
        "  l_t_len = 0\n",
        "\n",
        "  if show_progress:\n",
        "    print('GENERATING SUMMARY:')\n",
        "\n",
        "  while cur_output != EOS and (i < max_n or max_n == 0):\n",
        "    cur_output = next_symbol(cur_output_tokens, model)\n",
        "    cur_output_tokens.append(cur_output)\n",
        "    generated_output.append(cur_output)\n",
        "    text_gen = detokenizer(generated_output)\n",
        "    if show_progress:\n",
        "      clear_text = \"\\b\" * l_t_len + \" \" * l_t_len + \"\\b\" * l_t_len\n",
        "      print(clear_text, end=\"\", flush=True)\n",
        "      print(f'{text_gen}....({i})',end=\"\", flush=True)\n",
        "      l_t_len = len(text_gen) + len(f'....({i})')\n",
        "    i += 1\n",
        "\n",
        "  print('\\n -------------- \\n')\n",
        "  return text_gen"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "metadata": {
        "id": "QPXTliCcdnMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kHuIDGW1sOr",
        "outputId": "d4a41feb-e521-4767-ee34-c8fa36264574"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before the times of GPS and cell phones, maps were the main way to\n",
            "find one's way from point A to point B without getting lost. Nowadays,\n",
            "we have maps for everything from land elevation to animal location\n",
            "origins. Maps are fantastic things, just ask the cartographer at the\n",
            "local map store, he'll be sure to feel the same way. So if you're\n",
            "ready to take a look at 40 of the most interesting maps of America,\n",
            "keep reading! \n",
            "\n",
            "GENERATING SUMMARY: \n",
            " The U.S. The U.S. cell phones says the maps-maps-maps-maps- lost-maps was found to be a-year-year-year-year-year-year-....49\n"
          ]
        }
      ],
      "source": [
        "# Test it out on a sentence!\n",
        "test_sentence = \"Before the times of GPS and cell phones, maps were the main way to find one's way from point A to point B without getting lost. Nowadays, we have maps for everything from land elevation to animal location origins. Maps are fantastic things, just ask the cartographer at the local map store, he'll be sure to feel the same way. So if you're ready to take a look at 40 of the most interesting maps of America, keep reading!\"\n",
        "print(wrapper.fill(test_sentence), '\\n')\n",
        "print(greedy_decode(test_sentence, model, 50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYgX-mzjyUia",
        "outputId": "03d8739c-3aa6-4172-93f4-c62618a8639a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It’s the posing craze sweeping the U.S. after being brought to fame by\n",
            "skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert\n",
            "Pujols - and even Republican politician Rick Perry. But now four\n",
            "students at Riverhead High School on Long Island, New York, have been\n",
            "suspended for dropping to a knee and taking up a prayer pose to mimic\n",
            "Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel,\n",
            "Tyler Carroll and Connor Carroll were all suspended for one day\n",
            "because the ‘Tebowing’ craze was blocking the hallway and presenting a\n",
            "safety hazard to students. Scroll down for video. Banned: Jordan\n",
            "Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured\n",
            "left) were all suspended for one day by Riverhead High School on Long\n",
            "Island, New York, for their tribute to Broncos quarterback Tim Tebow.\n",
            "Issue: Four of the pupils were suspended for one day because they\n",
            "allegedly did not heed to warnings that the 'Tebowing' craze at the\n",
            "school was blocking the hallway and presenting a safety hazard to\n",
            "students. \n",
            "\n",
            "GENERATING SUMMARY: \n",
            " The U.S. The New York year-year-yaer high-school was found player to-be warning-warning year-year-year-year-....49\n"
          ]
        }
      ],
      "source": [
        "article = \"It’s the posing craze sweeping the U.S. after being brought to fame by skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert Pujols - and even Republican politician Rick Perry. But now four students at Riverhead High School on Long Island, New York, have been suspended for dropping to a knee and taking up a prayer pose to mimic Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were all suspended for one day because the ‘Tebowing’ craze was blocking the hallway and presenting a safety hazard to students. Scroll down for video. Banned: Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured left) were all suspended for one day by Riverhead High School on Long Island, New York, for their tribute to Broncos quarterback Tim Tebow. Issue: Four of the pupils were suspended for one day because they allegedly did not heed to warnings that the 'Tebowing' craze at the school was blocking the hallway and presenting a safety hazard to students.\"\n",
        "print(wrapper.fill(article), '\\n')\n",
        "print(greedy_decode(test_sentence, model, 50, show_progress=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article='The success of modern farming and plant breeding relies on accurate and efficient collection of data. For a commercial organization that manages large amounts of crops, collecting accurate and consistent data is a bottleneck. Due to limited time and labor, accurately phenotyping crops to record color, head count, height, weight, etc. is severely limited. However, this information, combined with other genetic and environmental factors, is vital for developing new superior crop species that help feed the world’s growing population. Recent advances in machine learning, in particular deep learning, have shown promise in mitigating this bottleneck. In this paper, we propose a novel deep learning method for counting on-ear corn kernels in-field to aid in the gathering of real-time data and, ultimately, to improve decision making to maximize yield. We name this approach DeepCorn, and show that this framework is robust under various conditions. DeepCorn estimates the density of corn kernels in an image of corn ears and predicts the number of kernels based on the estimated density map.'\n",
        "print(wrapper.fill(article), '\\n')\n",
        "print('GENERATING SUMMARY: \\n The method method method ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haz4WvClYlMo",
        "outputId": "07dabce0-316a-4843-e345-02b8c84c49fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The success of modern farming and plant breeding relies on accurate\n",
            "and efficient collection of data. For a commercial organization that\n",
            "manages large amounts of crops, collecting accurate and consistent\n",
            "data is a bottleneck. Due to limited time and labor, accurately\n",
            "phenotyping crops to record color, head count, height, weight, etc. is\n",
            "severely limited. However, this information, combined with other\n",
            "genetic and environmental factors, is vital for developing new\n",
            "superior crop species that help feed the world’s growing population.\n",
            "Recent advances in machine learning, in particular deep learning, have\n",
            "shown promise in mitigating this bottleneck. In this paper, we propose\n",
            "a novel deep learning method for counting on-ear corn kernels in-field\n",
            "to aid in the gathering of real-time data and, ultimately, to improve\n",
            "decision making to maximize yield. We name this approach DeepCorn, and\n",
            "show that this framework is robust under various conditions. DeepCorn\n",
            "estimates the density of corn kernels in an image of corn ears and\n",
            "predicts the number of kernels based on the estimated density map. \n",
            "\n",
            "GENERATING SUMMARY:\n",
            "The....(0)\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\bThe U....(1)\b\b\b\b\b\b\b\b\b\b\b\b            \b\b\b\b\b\b\b\b\b\b\b\bThe U.....(2)\b\b\b\b\b\b\b\b\b\b\b\b\b             \b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S....(3)\b\b\b\b\b\b\b\b\b\b\b\b\b\b              \b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S.....(4)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The....(5)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U....(6)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.....(7)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S....(8)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S.....(9)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The....(10)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U....(11)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.....(12)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S....(13)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S.....(14)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says....(15)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the....(16)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same....(17)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-....(18)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year....(19)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-....(20)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year....(21)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-....(22)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year....(23)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-....(24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year....(25)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-....(26)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year....(27)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-....(28)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-year....(29)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-....(30)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old....(31)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was....(32)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found....(33)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to....(34)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be....(35)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a....(36)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-....(37)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year....(38)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-....(39)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year....(40)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year-....(41)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year-year....(42)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year-year-....(43)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year-year-year....(44)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year-year-year-....(45)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year-year-year-year....(46)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year-year-year-year-....(47)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year-year-year-year-year....(48)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year-year-year-year-year-....(49)\n",
            " -------------- \n",
            "\n",
            "The U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year-year-year-year-year-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article='A convicted sex offender this week fatally shot his wife and her three children in their Oklahoma home – as well as two teenage girls who there were for a sleepover – before killing himself, police said Wednesday, accounting for the bodies’ discovery days earlier. Authorities found the bodies Monday at a property in Henryetta, a city about 90 miles from Oklahoma City, where 39-year-old registered sex offender Jesse L. McFadden lived with his wife, 35-year-old Holly McFadden, and her children, who were McFadden’s stepchildren: Rylee Allen, 17; Michael Mayo, 15; and Tiffany Guess, 13. The two other teen girls who were killed – 14-year-old Ivy Webster and 16-year-old Brittany Brewer – had been reported missing and in danger on Monday morning. The girls were Tiffany’s friends and spent the night with her on Saturday, and they were reported missing when they didn’t return home Sunday as planned, Okmulgee Police Chief Joe Prentice said. 16-year-old Brittany Brewer, left, and 14-year-old Ivy Webster, right, were the subject of an endangered/missing advisory issued by the Oklahoma Highway Patrol, which said they were last seen early Monday morning in Henryetta. A convicted sex offender and 2 teenage girls believed among 7 bodies found at his Oklahoma home “I follow the evidence … and the evidence is that Jesse McFadden murdered six people and then killed himself,” Prentice said during a news conference Wednesday. “I don’t have any evidence to indicate what the actual motive was.” The bodies of Holly and two of her children – Michael and Tiffany – were found together outside on the property, which the McFaddens were renting, Prentice said. The bodies of Ivy, Brittany and Rylee were found separately, about 150 yards apart from each other, he added. The scene “appeared to be staged,” the chief said, adding he believes the victims’ bodies were moved after they were killed. “There are questions that will never get answered because the only people who know are no longer here,” Prentice said. “We will continue to document everything that we have found and anything that we discover in follow-up interviews moving forward, and generate a report. We will submit that report to the district attorney’s office for her review as a formality, because there is no prosecution to be had here.” The firearm used in the killings was a handgun that Holly bought in 2022, Prentice said. Holly’s mother, Janette Mayo, told CNN affiliate KJRH in an on-air interview that her daughter was married to McFadden. She identified Holly and Holly’s three children as four of the victims.'\n",
        "print(wrapper.fill(article), '\\n')\n",
        "print(greedy_decode(test_sentence, model, 50, show_progress=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpSyswXzZPI-",
        "outputId": "a827be3d-9b61-4851-b9ae-b42e8420f118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A convicted sex offender this week fatally shot his wife and her three\n",
            "children in their Oklahoma home – as well as two teenage girls who\n",
            "there were for a sleepover – before killing himself, police said\n",
            "Wednesday, accounting for the bodies’ discovery days earlier.\n",
            "Authorities found the bodies Monday at a property in Henryetta, a city\n",
            "about 90 miles from Oklahoma City, where 39-year-old registered sex\n",
            "offender Jesse L. McFadden lived with his wife, 35-year-old Holly\n",
            "McFadden, and her children, who were McFadden’s stepchildren: Rylee\n",
            "Allen, 17; Michael Mayo, 15; and Tiffany Guess, 13. The two other teen\n",
            "girls who were killed – 14-year-old Ivy Webster and 16-year-old\n",
            "Brittany Brewer – had been reported missing and in danger on Monday\n",
            "morning. The girls were Tiffany’s friends and spent the night with her\n",
            "on Saturday, and they were reported missing when they didn’t return\n",
            "home Sunday as planned, Okmulgee Police Chief Joe Prentice said.\n",
            "16-year-old Brittany Brewer, left, and 14-year-old Ivy Webster, right,\n",
            "were the subject of an endangered/missing advisory issued by the\n",
            "Oklahoma Highway Patrol, which said they were last seen early Monday\n",
            "morning in Henryetta. A convicted sex offender and 2 teenage girls\n",
            "believed among 7 bodies found at his Oklahoma home “I follow the\n",
            "evidence … and the evidence is that Jesse McFadden murdered six people\n",
            "and then killed himself,” Prentice said during a news conference\n",
            "Wednesday. “I don’t have any evidence to indicate what the actual\n",
            "motive was.” The bodies of Holly and two of her children – Michael and\n",
            "Tiffany – were found together outside on the property, which the\n",
            "McFaddens were renting, Prentice said. The bodies of Ivy, Brittany and\n",
            "Rylee were found separately, about 150 yards apart from each other, he\n",
            "added. The scene “appeared to be staged,” the chief said, adding he\n",
            "believes the victims’ bodies were moved after they were killed. “There\n",
            "are questions that will never get answered because the only people who\n",
            "know are no longer here,” Prentice said. “We will continue to document\n",
            "everything that we have found and anything that we discover in follow-\n",
            "up interviews moving forward, and generate a report. We will submit\n",
            "that report to the district attorney’s office for her review as a\n",
            "formality, because there is no prosecution to be had here.” The\n",
            "firearm used in the killings was a handgun that Holly bought in 2022,\n",
            "Prentice said. Holly’s mother, Janette Mayo, told CNN affiliate KJRH\n",
            "in an on-air interview that her daughter was married to McFadden. She\n",
            "identified Holly and Holly’s three children as four of the victims. \n",
            "\n",
            "GENERATING SUMMARY:\n",
            "The....(0)\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\bThe U....(1)\b\b\b\b\b\b\b\b\b\b\b\b            \b\b\b\b\b\b\b\b\b\b\b\bThe U.....(2)\b\b\b\b\b\b\b\b\b\b\b\b\b             \b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S....(3)\b\b\b\b\b\b\b\b\b\b\b\b\b\b              \b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S.....(4)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The....(5)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U....(6)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.....(7)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S....(8)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S.....(9)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The....(10)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U....(11)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.....(12)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S....(13)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S.....(14)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says....(15)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the....(16)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same....(17)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-....(18)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year....(19)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-....(20)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year....(21)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-....(22)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year....(23)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-....(24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year....(25)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-....(26)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year....(27)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-....(28)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-year....(29)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-....(30)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old....(31)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was....(32)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found....(33)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to....(34)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be....(35)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a....(36)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-....(37)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year....(38)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-....(39)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year....(40)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year-....(41)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year-year....(42)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year-year-....(43)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year-year-year....(44)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year-year-year-....(45)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year-year-year-year....(46)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year-year-year-year-....(47)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year-year-year-year-year....(48)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year-year-year-year-year-....(49)\n",
            " -------------- \n",
            "\n",
            "The U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n",
            "year-old was found to be a-year-year-year-year-year-year-\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "coursera": {
      "schema_names": [
        "NLPC4-2"
      ]
    },
    "gpuClass": "standard",
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py:percent"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}